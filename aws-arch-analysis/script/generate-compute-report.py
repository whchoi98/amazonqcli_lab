#!/usr/bin/env python3
"""
í™•ì¥ëœ ì»´í“¨íŒ… ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (Python ë²„ì „)
ëª¨ë“  ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ì™€ Kubernetes ì›Œí¬ë¡œë“œ í¬í•¨
"""

import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from collections import Counter, defaultdict

class ExtendedComputeReportGenerator:
    def __init__(self, report_dir: str = "/home/ec2-user/amazonqcli_lab/aws-arch-analysis/report"):
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(parents=True, exist_ok=True)

    def load_json_file(self, filename: str) -> Optional[List[Dict[str, Any]]]:
        """JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        file_path = self.report_dir / filename
        try:
            if file_path.exists() and file_path.stat().st_size > 0:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict) and 'rows' in data:
                        return data['rows']
                    elif isinstance(data, list):
                        return data
                    return []
        except (json.JSONDecodeError, IOError) as e:
            print(f"Warning: Failed to load {filename}: {e}")
        return None

    def write_ec2_analysis(self, report_file, ec2_data: Optional[List]) -> None:
        """EC2 ì¸ìŠ¤í„´ìŠ¤ ë¶„ì„ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## ğŸ’» EC2 ì¸ìŠ¤í„´ìŠ¤ í˜„í™©\n\n")
        
        if not ec2_data:
            report_file.write("EC2 ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n")
            return
        
        # ê¸°ë³¸ í†µê³„
        total_instances = len(ec2_data)
        running_instances = len([i for i in ec2_data if i.get('instance_state') == 'running'])
        stopped_instances = len([i for i in ec2_data if i.get('instance_state') == 'stopped'])
        
        report_file.write("### ì¸ìŠ¤í„´ìŠ¤ ê°œìš”\n")
        report_file.write(f"**ì´ EC2 ì¸ìŠ¤í„´ìŠ¤:** {total_instances}ê°œ\n")
        report_file.write(f"- **ì‹¤í–‰ ì¤‘:** {running_instances}ê°œ\n")
        report_file.write(f"- **ì¤‘ì§€ë¨:** {stopped_instances}ê°œ\n")
        report_file.write(f"- **ê¸°íƒ€:** {total_instances - running_instances - stopped_instances}ê°œ\n\n")
        
        # ì „ì²´ ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ëª©ë¡
        report_file.write(f"### ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ëª©ë¡ (ì „ì²´ {total_instances}ê°œ)\n")
        report_file.write("| ì¸ìŠ¤í„´ìŠ¤ ID | íƒ€ì… | ìƒíƒœ | VPC ID | í”„ë¼ì´ë¹— IP | í¼ë¸”ë¦­ IP | íƒœê·¸ |\n")
        report_file.write("|-------------|------|------|--------|-------------|-----------|------|\n")
        
        for instance in ec2_data:
            instance_id = instance.get('instance_id', 'N/A')
            instance_type = instance.get('instance_type', 'N/A')
            state = instance.get('instance_state', 'N/A')
            vpc_id = instance.get('vpc_id', 'N/A')
            private_ip = instance.get('private_ip_address', 'N/A')
            public_ip = instance.get('public_ip_address', 'N/A')
            tag_name = instance.get('tags', {}).get('Name', 'N/A') if instance.get('tags') else 'N/A'
            
            report_file.write(f"| {instance_id} | {instance_type} | {state} | {vpc_id} | {private_ip} | {public_ip} | {tag_name} |\n")
        
        # ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ ë¶„í¬
        report_file.write("\n### ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ë³„ ë¶„í¬\n")
        report_file.write("| ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… | ê°œìˆ˜ | ë¹„ìœ¨ |\n")
        report_file.write("|---------------|------|------|\n")
        
        type_counter = Counter(i.get('instance_type', 'Unknown') for i in ec2_data)
        for instance_type, count in type_counter.most_common():
            percentage = round((count / total_instances) * 100, 1)
            report_file.write(f"| {instance_type} | {count} | {percentage}% |\n")
        
        # VPCë³„ ë¶„í¬
        report_file.write("\n### VPCë³„ ì¸ìŠ¤í„´ìŠ¤ ë¶„í¬\n")
        report_file.write("| VPC ID | ê°œìˆ˜ | ë¹„ìœ¨ |\n")
        report_file.write("|--------|------|------|\n")
        
        vpc_counter = Counter(i.get('vpc_id', 'Unknown') for i in ec2_data)
        for vpc_id, count in vpc_counter.most_common():
            percentage = round((count / total_instances) * 100, 1)
            report_file.write(f"| {vpc_id} | {count} | {percentage}% |\n")

    def write_autoscaling_analysis(self, report_file, asg_data: Optional[List]) -> None:
        """Auto Scaling ë¶„ì„ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## âš–ï¸ Auto Scaling ê·¸ë£¹ ë¶„ì„\n\n")
        
        if not asg_data:
            report_file.write("Auto Scaling ê·¸ë£¹ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n")
            return
        
        total_asgs = len(asg_data)
        total_desired = sum(asg.get('desired_capacity', 0) for asg in asg_data)
        total_min = sum(asg.get('min_size', 0) for asg in asg_data)
        total_max = sum(asg.get('max_size', 0) for asg in asg_data)
        
        report_file.write("### Auto Scaling ê°œìš”\n")
        report_file.write(f"**ì´ Auto Scaling ê·¸ë£¹:** {total_asgs}ê°œ\n")
        report_file.write(f"- **ì´ í¬ë§ ìš©ëŸ‰:** {total_desired}ê°œ\n")
        report_file.write(f"- **ì´ ìµœì†Œ ìš©ëŸ‰:** {total_min}ê°œ\n")
        report_file.write(f"- **ì´ ìµœëŒ€ ìš©ëŸ‰:** {total_max}ê°œ\n\n")
        
        # Auto Scaling ê·¸ë£¹ ìƒì„¸ ëª©ë¡
        report_file.write("### Auto Scaling ê·¸ë£¹ ìƒì„¸\n")
        report_file.write("| ASG ì´ë¦„ | ìµœì†Œ | í¬ë§ | ìµœëŒ€ | í—¬ìŠ¤ì²´í¬ | ê°€ìš©ì˜ì—­ |\n")
        report_file.write("|----------|------|------|------|----------|----------|\n")
        
        for asg in asg_data:
            name = asg.get('name', 'N/A')
            min_size = asg.get('min_size', 0)
            desired = asg.get('desired_capacity', 0)
            max_size = asg.get('max_size', 0)
            health_check = asg.get('health_check_type', 'N/A')
            azs = ', '.join(asg.get('availability_zones', [])[:2])  # ì²˜ìŒ 2ê°œë§Œ í‘œì‹œ
            if len(asg.get('availability_zones', [])) > 2:
                azs += '...'
            
            report_file.write(f"| {name} | {min_size} | {desired} | {max_size} | {health_check} | {azs} |\n")

    def write_loadbalancer_analysis(self, report_file, alb_data: Optional[List], nlb_data: Optional[List], target_groups: Optional[List]) -> None:
        """ë¡œë“œ ë°¸ëŸ°ì„œ ë¶„ì„ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## ğŸ”„ ë¡œë“œ ë°¸ëŸ°ì„œ ë¶„ì„\n\n")
        
        alb_count = len(alb_data) if alb_data else 0
        nlb_count = len(nlb_data) if nlb_data else 0
        tg_count = len(target_groups) if target_groups else 0
        
        report_file.write("### ë¡œë“œ ë°¸ëŸ°ì„œ ê°œìš”\n")
        report_file.write(f"**Application Load Balancer:** {alb_count}ê°œ\n")
        report_file.write(f"**Network Load Balancer:** {nlb_count}ê°œ\n")
        report_file.write(f"**íƒ€ê²Ÿ ê·¸ë£¹:** {tg_count}ê°œ\n\n")
        
        # ALB ìƒì„¸ ì •ë³´
        if alb_data:
            report_file.write("### Application Load Balancer ìƒì„¸\n")
            report_file.write("| ALB ì´ë¦„ | ìŠ¤í‚´ | VPC ID | ìƒíƒœ | DNS ì´ë¦„ |\n")
            report_file.write("|----------|------|--------|------|----------|\n")
            
            for alb in alb_data:
                name = alb.get('name', 'N/A')
                scheme = alb.get('scheme', 'N/A')
                vpc_id = alb.get('vpc_id', 'N/A')
                state = alb.get('state_code', 'N/A')
                dns_name = alb.get('dns_name', 'N/A')[:50] + '...' if len(alb.get('dns_name', '')) > 50 else alb.get('dns_name', 'N/A')
                
                report_file.write(f"| {name} | {scheme} | {vpc_id} | {state} | {dns_name} |\n")
        
        # íƒ€ê²Ÿ ê·¸ë£¹ ìƒì„¸ ì •ë³´
        if target_groups:
            report_file.write("\n### íƒ€ê²Ÿ ê·¸ë£¹ ìƒì„¸\n")
            report_file.write("| íƒ€ê²Ÿ ê·¸ë£¹ ì´ë¦„ | í”„ë¡œí† ì½œ | í¬íŠ¸ | VPC ID | í—¬ìŠ¤ì²´í¬ ê²½ë¡œ |\n")
            report_file.write("|----------------|----------|------|--------|---------------|\n")
            
            for tg in target_groups:
                name = tg.get('target_group_name', 'N/A')
                protocol = tg.get('protocol', 'N/A')
                port = tg.get('port', 'N/A')
                vpc_id = tg.get('vpc_id', 'N/A')
                health_path = tg.get('health_check_path', 'N/A')
                
                report_file.write(f"| {name} | {protocol} | {port} | {vpc_id} | {health_path} |\n")

    def write_container_analysis(self, report_file, eks_clusters: Optional[List], eks_nodes: Optional[List]) -> None:
        """ì»¨í…Œì´ë„ˆ ì„œë¹„ìŠ¤ ë¶„ì„ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## ğŸ“¦ ì»¨í…Œì´ë„ˆ ì„œë¹„ìŠ¤ ë¶„ì„\n\n")
        
        eks_cluster_count = len(eks_clusters) if eks_clusters else 0
        eks_node_count = len(eks_nodes) if eks_nodes else 0
        
        report_file.write("### ì»¨í…Œì´ë„ˆ ì„œë¹„ìŠ¤ ê°œìš”\n")
        report_file.write(f"**EKS í´ëŸ¬ìŠ¤í„°:** {eks_cluster_count}ê°œ\n")
        report_file.write(f"**EKS ë…¸ë“œ ê·¸ë£¹:** {eks_node_count}ê°œ\n\n")
        
        # EKS í´ëŸ¬ìŠ¤í„° ìƒì„¸ ì •ë³´ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)
        if eks_cluster_count > 0:
            # ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ë°ì´í„° íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œ
            eks_data = self.load_json_file("compute_eks_clusters.json")
            if not eks_data:
                # ëŒ€ì²´ ë°ì´í„° ì†ŒìŠ¤ ì‹œë„
                eks_data = self.load_json_file("compute_eks_clusters.json") or []
            
            if eks_data:
                report_file.write("### EKS í´ëŸ¬ìŠ¤í„° ìƒì„¸\n")
                report_file.write("| í´ëŸ¬ìŠ¤í„° ì´ë¦„ | ë²„ì „ | ìƒíƒœ | ì—”ë“œí¬ì¸íŠ¸ | ìƒì„±ì¼ |\n")
                report_file.write("|---------------|------|------|------------|--------|\n")
                
                for cluster in eks_data:
                    name = cluster.get('name', 'N/A')
                    version = cluster.get('version', 'N/A')
                    status = cluster.get('status', 'N/A')
                    endpoint = cluster.get('endpoint', 'N/A')[:30] + '...' if len(cluster.get('endpoint', '')) > 30 else cluster.get('endpoint', 'N/A')
                    created_at = cluster.get('created_at', 'N/A')[:10] if cluster.get('created_at') else 'N/A'
                    
                    report_file.write(f"| {name} | {version} | {status} | {endpoint} | {created_at} |\n")
            else:
                report_file.write("EKS í´ëŸ¬ìŠ¤í„°ê°€ ê°ì§€ë˜ì—ˆì§€ë§Œ ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n")

    def write_kubernetes_analysis(self, report_file) -> None:
        """Kubernetes ë¦¬ì†ŒìŠ¤ ë¶„ì„ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## â˜¸ï¸ Kubernetes ì›Œí¬ë¡œë“œ ë¶„ì„\n\n")
        
        # K8s ë¦¬ì†ŒìŠ¤ ë°ì´í„° ë¡œë“œ
        namespaces = self.load_json_file("k8s_namespaces.json")
        deployments = self.load_json_file("k8s_deployments.json")
        nodes = self.load_json_file("k8s_nodes.json")
        configmaps = self.load_json_file("k8s_configmaps.json")
        daemonsets = self.load_json_file("k8s_daemonsets.json")
        
        # ê¸°ë³¸ í†µê³„
        ns_count = len(namespaces) if namespaces else 0
        deploy_count = len(deployments) if deployments else 0
        node_count = len(nodes) if nodes else 0
        cm_count = len(configmaps) if configmaps else 0
        ds_count = len(daemonsets) if daemonsets else 0
        
        report_file.write("### Kubernetes ë¦¬ì†ŒìŠ¤ ê°œìš”\n")
        report_file.write(f"**ë„¤ì„ìŠ¤í˜ì´ìŠ¤:** {ns_count}ê°œ\n")
        report_file.write(f"**ë””í”Œë¡œì´ë¨¼íŠ¸:** {deploy_count}ê°œ\n")
        report_file.write(f"**ë…¸ë“œ:** {node_count}ê°œ\n")
        report_file.write(f"**ì»¨í”¼ê·¸ë§µ:** {cm_count}ê°œ\n")
        report_file.write(f"**ë°ëª¬ì…‹:** {ds_count}ê°œ\n\n")
        
        # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„¸
        if namespaces:
            report_file.write("### ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ëª©ë¡\n")
            report_file.write("| ë„¤ì„ìŠ¤í˜ì´ìŠ¤ | ìƒì„±ì¼ | ë ˆì´ë¸” |\n")
            report_file.write("|--------------|--------|--------|\n")
            
            for ns in namespaces:
                name = ns.get('name', 'N/A')
                created = ns.get('creation_timestamp', 'N/A')[:10] if ns.get('creation_timestamp') else 'N/A'
                labels = ', '.join([f"{k}={v}" for k, v in (ns.get('labels', {}) or {}).items()][:2])
                if len(ns.get('labels', {}) or {}) > 2:
                    labels += '...'
                
                report_file.write(f"| {name} | {created} | {labels} |\n")
        
        # ë””í”Œë¡œì´ë¨¼íŠ¸ ìƒì„¸
        if deployments:
            report_file.write("\n### ë””í”Œë¡œì´ë¨¼íŠ¸ ìƒì„¸\n")
            report_file.write("| ë””í”Œë¡œì´ë¨¼íŠ¸ | ë„¤ì„ìŠ¤í˜ì´ìŠ¤ | ë³µì œë³¸ | ì¤€ë¹„ëœ ë³µì œë³¸ | ì‚¬ìš© ê°€ëŠ¥í•œ ë³µì œë³¸ |\n")
            report_file.write("|--------------|--------------|--------|----------------|--------------------|\n")
            
            for deploy in deployments:
                name = deploy.get('name', 'N/A')
                namespace = deploy.get('namespace', 'N/A')
                replicas = deploy.get('replicas', 'N/A')
                ready_replicas = deploy.get('ready_replicas', 'N/A')
                available_replicas = deploy.get('available_replicas', 'N/A')
                
                report_file.write(f"| {name} | {namespace} | {replicas} | {ready_replicas} | {available_replicas} |\n")
        
        # ë…¸ë“œ ìƒì„¸
        if nodes:
            report_file.write("\n### ë…¸ë“œ ìƒì„¸\n")
            report_file.write("| ë…¸ë“œ ì´ë¦„ | ìƒì„±ì¼ | ë ˆì´ë¸” (ì¼ë¶€) |\n")
            report_file.write("|-----------|--------|---------------|\n")
            
            for node in nodes:
                name = node.get('name', 'N/A')
                created = node.get('creation_timestamp', 'N/A')[:10] if node.get('creation_timestamp') else 'N/A'
                labels = ', '.join([f"{k}={v}" for k, v in (node.get('labels', {}) or {}).items() if not k.startswith('kubernetes.io')][:2])
                if not labels:
                    labels = 'N/A'
                
                report_file.write(f"| {name} | {created} | {labels} |\n")

    def write_serverless_analysis(self, report_file) -> None:
        """ì„œë²„ë¦¬ìŠ¤ ì»´í“¨íŒ… ë¶„ì„ ì„¹ì…˜ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## ğŸš€ ì„œë²„ë¦¬ìŠ¤ ì»´í“¨íŒ… ë¶„ì„\n\n")
        
        # Lambda í•¨ìˆ˜ ë°ì´í„° ë¡œë“œ ì‹œë„
        lambda_functions = self.load_json_file("compute_lambda_functions.json")
        
        if lambda_functions:
            lambda_count = len(lambda_functions)
            report_file.write("### Lambda í•¨ìˆ˜ ê°œìš”\n")
            report_file.write(f"**ì´ Lambda í•¨ìˆ˜:** {lambda_count}ê°œ\n\n")
            
            # ëŸ°íƒ€ì„ë³„ ë¶„í¬
            runtime_counter = Counter(func.get('runtime', 'Unknown') for func in lambda_functions)
            report_file.write("### ëŸ°íƒ€ì„ë³„ ë¶„í¬\n")
            report_file.write("| ëŸ°íƒ€ì„ | ê°œìˆ˜ | ë¹„ìœ¨ |\n")
            report_file.write("|--------|------|------|\n")
            
            for runtime, count in runtime_counter.most_common():
                percentage = round((count / lambda_count) * 100, 1)
                report_file.write(f"| {runtime} | {count} | {percentage}% |\n")
            
            # Lambda í•¨ìˆ˜ ìƒì„¸ ëª©ë¡
            report_file.write("\n### Lambda í•¨ìˆ˜ ìƒì„¸\n")
            report_file.write("| í•¨ìˆ˜ ì´ë¦„ | ëŸ°íƒ€ì„ | ë©”ëª¨ë¦¬ | íƒ€ì„ì•„ì›ƒ | ë§ˆì§€ë§‰ ìˆ˜ì •ì¼ |\n")
            report_file.write("|-----------|---------|--------|----------|---------------|\n")
            
            for func in lambda_functions:
                name = func.get('name', 'N/A')
                runtime = func.get('runtime', 'N/A')
                memory = func.get('memory_size', 'N/A')
                timeout = func.get('timeout', 'N/A')
                last_modified = func.get('last_modified', 'N/A')[:10] if func.get('last_modified') else 'N/A'
                
                report_file.write(f"| {name} | {runtime} | {memory}MB | {timeout}s | {last_modified} |\n")
        else:
            report_file.write("Lambda í•¨ìˆ˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n")

    def write_recommendations(self, report_file) -> None:
        """ì»´í“¨íŒ… ìµœì í™” ê¶Œì¥ì‚¬í•­ì„ ì‘ì„±í•©ë‹ˆë‹¤."""
        report_file.write("## ğŸ“‹ ì»´í“¨íŒ… ìµœì í™” ê¶Œì¥ì‚¬í•­\n\n")
        
        report_file.write("### ğŸ”´ ë†’ì€ ìš°ì„ ìˆœìœ„\n")
        report_file.write("1. **ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ìµœì í™”**: ì›Œí¬ë¡œë“œì— ë§ëŠ” ì ì ˆí•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ì„ íƒ\n")
        report_file.write("2. **Auto Scaling ì •ì±… ê²€í† **: íŠ¸ë˜í”½ íŒ¨í„´ì— ë§ëŠ” ìŠ¤ì¼€ì¼ë§ ì •ì±… ì„¤ì •\n")
        report_file.write("3. **ë¯¸ì‚¬ìš© ë¦¬ì†ŒìŠ¤ ì •ë¦¬**: ì¤‘ì§€ëœ ì¸ìŠ¤í„´ìŠ¤ ë° ë¯¸ì‚¬ìš© ë¡œë“œ ë°¸ëŸ°ì„œ ì œê±°\n\n")
        
        report_file.write("### ğŸŸ¡ ì¤‘ê°„ ìš°ì„ ìˆœìœ„\n")
        report_file.write("1. **ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ í™œìš©**: ì ì ˆí•œ ì›Œí¬ë¡œë“œì— ìŠ¤íŒŸ ì¸ìŠ¤í„´ìŠ¤ ë„ì…\n")
        report_file.write("2. **ì»¨í…Œì´ë„ˆí™” ê²€í† **: ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜ë¡œ ì „í™˜ ê³ ë ¤\n")
        report_file.write("3. **ì„œë²„ë¦¬ìŠ¤ ì „í™˜**: ì´ë²¤íŠ¸ ê¸°ë°˜ ì›Œí¬ë¡œë“œì˜ Lambda ì „í™˜\n\n")
        
        report_file.write("### ğŸŸ¢ ë‚®ì€ ìš°ì„ ìˆœìœ„\n")
        report_file.write("1. **ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ êµ¬ë§¤**: ì¥ê¸° ì‹¤í–‰ ì›Œí¬ë¡œë“œì— ëŒ€í•œ RI êµ¬ë§¤\n")
        report_file.write("2. **Kubernetes ìµœì í™”**: ë¦¬ì†ŒìŠ¤ ìš”ì²­/ì œí•œ ì„¤ì • ìµœì í™”\n")
        report_file.write("3. **ëª¨ë‹ˆí„°ë§ ê°•í™”**: CloudWatch ë©”íŠ¸ë¦­ ë° ì•ŒëŒ ì„¤ì •\n\n")

    def generate_report(self):
        """í™•ì¥ëœ ì»´í“¨íŒ… ë¶„ì„ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print("ğŸ’» í™•ì¥ëœ Compute Analysis ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ë°ì´í„° íŒŒì¼ ë¡œë“œ
        ec2_data = self.load_json_file("compute_ec2_instances.json")
        asg_data = self.load_json_file("compute_asg_detailed.json")
        alb_data = self.load_json_file("compute_alb_detailed.json")
        nlb_data = self.load_json_file("compute_nlb_detailed.json")
        target_groups = self.load_json_file("compute_target_groups.json")
        
        # ë³´ê³ ì„œ íŒŒì¼ ìƒì„±
        report_path = self.report_dir / "03-compute-analysis.md"
        
        try:
            with open(report_path, 'w', encoding='utf-8') as report_file:
                # í—¤ë” ì‘ì„±
                report_file.write("# ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ë¶„ì„\n\n")
                
                # ê° ì„¹ì…˜ ì‘ì„±
                self.write_ec2_analysis(report_file, ec2_data)
                self.write_autoscaling_analysis(report_file, asg_data)
                self.write_loadbalancer_analysis(report_file, alb_data, nlb_data, target_groups)
                self.write_container_analysis(report_file, None, None)  # EKS ë°ì´í„°ëŠ” ë³„ë„ ì²˜ë¦¬
                self.write_kubernetes_analysis(report_file)
                self.write_serverless_analysis(report_file)
                self.write_recommendations(report_file)
                
                # ë§ˆë¬´ë¦¬
                report_file.write("---\n")
                report_file.write("*ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ ë¶„ì„ ì™„ë£Œ*\n")
            
            print("âœ… í™•ì¥ëœ Compute Analysis ìƒì„± ì™„ë£Œ: 03-compute-analysis.md")
            
        except IOError as e:
            print(f"âŒ ë³´ê³ ì„œ íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
            sys.exit(1)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í™•ì¥ëœ ì»´í“¨íŒ… ë¶„ì„ ë³´ê³ ì„œ ìƒì„±")
    parser.add_argument("--report-dir", default="/home/ec2-user/amazonqcli_lab/aws-arch-analysis/report", help="ë³´ê³ ì„œ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    generator = ExtendedComputeReportGenerator(args.report_dir)
    generator.generate_report()

if __name__ == "__main__":
    main()
