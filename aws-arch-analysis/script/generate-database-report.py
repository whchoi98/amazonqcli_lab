#!/usr/bin/env python3
"""
Enhanced Database Analysis Report Generator
Shell ìŠ¤í¬ë¦½íŠ¸ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ Pythonìœ¼ë¡œ êµ¬í˜„í•œ ê³ ë„í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±ê¸°
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import logging

class DatabaseReportGenerator:
    def __init__(self, report_dir: str = "/home/ec2-user/amazonqcli_lab/aws-arch-analysis/report"):
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(parents=True, exist_ok=True)
        self.current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        self.data_files = {
            'rds_instances': 'database_rds_instances.json',
            'rds_clusters': 'database_rds_clusters.json',
            'rds_subnet_groups': 'database_rds_subnet_groups.json',
            'rds_parameter_groups': 'database_rds_parameter_groups.json',
            'elasticache_clusters': 'database_elasticache_clusters.json',
            'elasticache_replication_groups': 'database_elasticache_replication_groups.json',
            'opensearch_domains': 'database_opensearch_domains.json',
            'athena_workgroups': 'database_athena_workgroups.json',
            'dynamodb_tables': 'database_dynamodb_tables.json'
        }
        
        # ì„œë¹„ìŠ¤ í†µê³„ ì´ˆê¸°í™”
        self.service_stats = {
            'rds_instances': 0,
            'rds_clusters': 0,
            'elasticache_clusters': 0,
            'opensearch_domains': 0,
            'athena_workgroups': 0,
            'active_services': 0,
            'total_services': 5
        }

    def load_json_data(self, filename: str) -> Optional[Dict]:
        """JSON ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        file_path = self.report_dir / filename
        try:
            if file_path.exists() and file_path.stat().st_size > 16:  # ë¹ˆ íŒŒì¼ì´ ì•„ë‹Œ ê²½ìš°
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if data.get('rows') and len(data['rows']) > 0:
                        return data
            return None
        except (json.JSONDecodeError, FileNotFoundError) as e:
            self.logger.warning(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
            return None

    def calculate_percentage(self, part: int, total: int) -> str:
        """ë°±ë¶„ìœ¨ ê³„ì‚°"""
        if total == 0:
            return "0.0"
        return f"{(part * 100 / total):.1f}"

    def generate_executive_summary(self) -> str:
        """Executive Summary ìƒì„±"""
        # ì„œë¹„ìŠ¤ë³„ ë°ì´í„° ë¡œë“œ ë° ì¹´ìš´íŠ¸
        rds_instances_data = self.load_json_data(self.data_files['rds_instances'])
        rds_clusters_data = self.load_json_data(self.data_files['rds_clusters'])
        elasticache_clusters_data = self.load_json_data(self.data_files['elasticache_clusters'])
        opensearch_domains_data = self.load_json_data(self.data_files['opensearch_domains'])
        athena_workgroups_data = self.load_json_data(self.data_files['athena_workgroups'])
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.service_stats['rds_instances'] = len(rds_instances_data['rows']) if rds_instances_data else 0
        self.service_stats['rds_clusters'] = len(rds_clusters_data['rows']) if rds_clusters_data else 0
        self.service_stats['elasticache_clusters'] = len(elasticache_clusters_data['rows']) if elasticache_clusters_data else 0
        self.service_stats['opensearch_domains'] = len(opensearch_domains_data['rows']) if opensearch_domains_data else 0
        self.service_stats['athena_workgroups'] = len(athena_workgroups_data['rows']) if athena_workgroups_data else 0
        
        # í™œì„± ì„œë¹„ìŠ¤ ì¹´ìš´íŠ¸
        active_services = sum([
            1 if self.service_stats['rds_instances'] > 0 else 0,
            1 if self.service_stats['rds_clusters'] > 0 else 0,
            1 if self.service_stats['elasticache_clusters'] > 0 else 0,
            1 if self.service_stats['opensearch_domains'] > 0 else 0,
            1 if self.service_stats['athena_workgroups'] > 0 else 0
        ])
        self.service_stats['active_services'] = active_services
        
        summary = f"""# ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ì¢…í•© ë¶„ì„

> **ë¶„ì„ ì¼ì‹œ**: {self.current_time}  
> **ë¶„ì„ ëŒ€ìƒ**: AWS ê³„ì • ë‚´ ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤  
> **ë¶„ì„ ë¦¬ì „**: ap-northeast-2 (ì„œìš¸)

## ğŸ“Š Executive Summary

### ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤ í˜„í™© ê°œìš”

| ì„œë¹„ìŠ¤ | ë¦¬ì†ŒìŠ¤ ìˆ˜ | ìƒíƒœ |
|--------|-----------|------|
| ğŸ›ï¸ RDS ì¸ìŠ¤í„´ìŠ¤ | {self.service_stats['rds_instances']}ê°œ | {'âœ… í™œì„±' if self.service_stats['rds_instances'] > 0 else 'âŒ ì—†ìŒ'} |
| ğŸ›ï¸ RDS í´ëŸ¬ìŠ¤í„° (Aurora) | {self.service_stats['rds_clusters']}ê°œ | {'âœ… í™œì„±' if self.service_stats['rds_clusters'] > 0 else 'âŒ ì—†ìŒ'} |
| âš¡ ElastiCache í´ëŸ¬ìŠ¤í„° | {self.service_stats['elasticache_clusters']}ê°œ | {'âœ… í™œì„±' if self.service_stats['elasticache_clusters'] > 0 else 'âŒ ì—†ìŒ'} |
| ğŸ” OpenSearch ë„ë©”ì¸ | {self.service_stats['opensearch_domains']}ê°œ | {'âœ… í™œì„±' if self.service_stats['opensearch_domains'] > 0 else 'âŒ ì—†ìŒ'} |
| ğŸ“Š Athena ì›Œí¬ê·¸ë£¹ | {self.service_stats['athena_workgroups']}ê°œ | {'âœ… í™œì„±' if self.service_stats['athena_workgroups'] > 0 else 'âŒ ì—†ìŒ'} |

**í™œì„± ë°ì´í„°ë² ì´ìŠ¤ ì„œë¹„ìŠ¤**: {active_services}/{self.service_stats['total_services']}ê°œ

---

"""
        return summary

    def analyze_rds_instances(self) -> str:
        """RDS ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ë¶„ì„"""
        rds_data = self.load_json_data(self.data_files['rds_instances'])
        
        if not rds_data:
            return """## ğŸ›ï¸ Amazon RDS ìƒì„¸ ë¶„ì„

### RDS ì¸ìŠ¤í„´ìŠ¤ í˜„í™©
âŒ RDS ì¸ìŠ¤í„´ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

"""
        
        instances = rds_data['rows']
        total_count = len(instances)
        available_count = len([i for i in instances if i.get('status') == 'available'])
        encrypted_count = len([i for i in instances if i.get('storage_encrypted') == True])
        multi_az_count = len([i for i in instances if i.get('multi_az') == True])
        public_count = len([i for i in instances if i.get('publicly_accessible') == True])
        
        # ì—”ì§„ë³„ ë¶„í¬ ê³„ì‚°
        engine_stats = {}
        for instance in instances:
            engine = instance.get('engine', 'Unknown')
            if engine not in engine_stats:
                engine_stats[engine] = {'count': 0, 'version': instance.get('engine_version', 'N/A')}
            engine_stats[engine]['count'] += 1
        
        analysis = f"""## ğŸ›ï¸ Amazon RDS ìƒì„¸ ë¶„ì„

### RDS ì¸ìŠ¤í„´ìŠ¤ í˜„í™©

**ğŸ“ˆ RDS ì¸ìŠ¤í„´ìŠ¤ í†µê³„**
- **ì´ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜**: {total_count}ê°œ
- **ì‚¬ìš© ê°€ëŠ¥í•œ ì¸ìŠ¤í„´ìŠ¤**: {available_count}ê°œ ({self.calculate_percentage(available_count, total_count)}%)
- **ì•”í˜¸í™”ëœ ì¸ìŠ¤í„´ìŠ¤**: {encrypted_count}ê°œ ({self.calculate_percentage(encrypted_count, total_count)}%)
- **Multi-AZ êµ¬ì„±**: {multi_az_count}ê°œ ({self.calculate_percentage(multi_az_count, total_count)}%)

#### ğŸ“‹ RDS ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ëª©ë¡

| DB ì‹ë³„ì | ì—”ì§„ | ë²„ì „ | í´ë˜ìŠ¤ | ìŠ¤í† ë¦¬ì§€ | ìƒíƒœ | Multi-AZ | ì•”í˜¸í™” | ê³µê°œ ì ‘ê·¼ |
|-----------|------|------|-------|----------|------|----------|--------|-----------|
"""
        
        # ì¸ìŠ¤í„´ìŠ¤ ìƒì„¸ ëª©ë¡
        for instance in instances:
            db_id = instance.get('db_instance_identifier', 'N/A')
            engine = instance.get('engine', 'N/A')
            version = instance.get('engine_version', 'N/A')
            instance_class = instance.get('class', 'N/A')
            storage = f"{instance.get('allocated_storage', 0)}GB ({instance.get('storage_type', 'N/A')})"
            status = instance.get('status', 'N/A')
            multi_az = "âœ…" if instance.get('multi_az') else "âŒ"
            encrypted = "ğŸ”’" if instance.get('storage_encrypted') else "ğŸ”“"
            public = "ğŸŒ" if instance.get('publicly_accessible') else "ğŸ”’"
            
            analysis += f"| {db_id} | {engine} | {version} | {instance_class} | {storage} | {status} | {multi_az} | {encrypted} | {public} |\n"
        
        # ì—”ì§„ë³„ ë¶„í¬
        analysis += f"""
#### ğŸ”§ ì—”ì§„ë³„ ë¶„í¬ ë° ë²„ì „ ë¶„ì„

| ì—”ì§„ | ê°œìˆ˜ | ìµœì‹  ë²„ì „ | ê¶Œì¥ì‚¬í•­ |
|------|------|-----------|----------|
"""
        
        for engine, stats in engine_stats.items():
            analysis += f"| {engine} | {stats['count']}ê°œ | {stats['version']} | ë²„ì „ ì—…ë°ì´íŠ¸ ê²€í†  |\n"
        
        # ë³´ì•ˆ ì„¤ì • ë¶„ì„
        analysis += f"""
#### ğŸ” ë³´ì•ˆ ì„¤ì • ë¶„ì„

**ì•”í˜¸í™” í˜„í™©**:
- ì €ì¥ ì‹œ ì•”í˜¸í™”: {encrypted_count}/{total_count}ê°œ ì¸ìŠ¤í„´ìŠ¤
- ê¶Œì¥ì‚¬í•­: {'âœ… ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ê°€ ì•”í˜¸í™”ë¨' if encrypted_count == total_count else 'âš ï¸ ì•”í˜¸í™”ë˜ì§€ ì•Šì€ ì¸ìŠ¤í„´ìŠ¤ ì¡´ì¬'}

**ë„¤íŠ¸ì›Œí¬ ë³´ì•ˆ**:
- ê³µê°œ ì ‘ê·¼ ê°€ëŠ¥: {public_count}/{total_count}ê°œ ì¸ìŠ¤í„´ìŠ¤
- ê¶Œì¥ì‚¬í•­: {'âœ… ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ê°€ ë¹„ê³µê°œ' if public_count == 0 else 'âš ï¸ ê³µê°œ ì ‘ê·¼ ê°€ëŠ¥í•œ ì¸ìŠ¤í„´ìŠ¤ ê²€í†  í•„ìš”'}

"""
        return analysis

    def analyze_rds_clusters(self) -> str:
        """RDS í´ëŸ¬ìŠ¤í„° (Aurora) ìƒì„¸ ë¶„ì„"""
        clusters_data = self.load_json_data(self.data_files['rds_clusters'])
        
        if not clusters_data:
            return """### RDS í´ëŸ¬ìŠ¤í„° (Aurora) ë¶„ì„
âŒ RDS í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

"""
        
        clusters = clusters_data['rows']
        total_count = len(clusters)
        available_count = len([c for c in clusters if c.get('status') == 'available'])
        encrypted_count = len([c for c in clusters if c.get('storage_encrypted') == True])
        
        analysis = f"""### RDS í´ëŸ¬ìŠ¤í„° (Aurora) ë¶„ì„

**ğŸ“ˆ Aurora í´ëŸ¬ìŠ¤í„° í†µê³„**
- **ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜**: {total_count}ê°œ
- **ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°**: {available_count}ê°œ
- **ì•”í˜¸í™”ëœ í´ëŸ¬ìŠ¤í„°**: {encrypted_count}ê°œ ({self.calculate_percentage(encrypted_count, total_count)}%)

#### ğŸ“‹ Aurora í´ëŸ¬ìŠ¤í„° ìƒì„¸ ëª©ë¡

| í´ëŸ¬ìŠ¤í„° ì‹ë³„ì | ì—”ì§„ | ë²„ì „ | ìƒíƒœ | ë©¤ë²„ ìˆ˜ | ë°±ì—… ë³´ì¡´ | ì•”í˜¸í™” | ì—”ë“œí¬ì¸íŠ¸ |
|-----------------|------|------|------|---------|-----------|--------|------------|
"""
        
        # í´ëŸ¬ìŠ¤í„° ìƒì„¸ ëª©ë¡
        for cluster in clusters:
            cluster_id = cluster.get('db_cluster_identifier', 'N/A')
            engine = cluster.get('engine', 'N/A')
            version = cluster.get('engine_version', 'N/A')
            status = cluster.get('status', 'N/A')
            members_count = len(cluster.get('members', []))
            backup_retention = f"{cluster.get('backup_retention_period', 0)}ì¼"
            encrypted = "ğŸ”’" if cluster.get('storage_encrypted') else "ğŸ”“"
            endpoint = cluster.get('endpoint', 'N/A')
            
            analysis += f"| {cluster_id} | {engine} | {version} | {status} | {members_count} | {backup_retention} | {encrypted} | {endpoint} |\n"
        
        # ë°±ì—… ì„¤ì • ë¶„ì„
        backup_periods = {}
        for cluster in clusters:
            period = cluster.get('backup_retention_period', 0)
            backup_periods[period] = backup_periods.get(period, 0) + 1
        
        analysis += f"""
#### ğŸ”„ ë°±ì—… ë° ë³µêµ¬ ì„¤ì •

**ë°±ì—… ë³´ì¡´ ê¸°ê°„ ë¶„ì„**:
"""
        
        for period, count in backup_periods.items():
            analysis += f"- {period}ì¼: {count}ê°œ í´ëŸ¬ìŠ¤í„°\n"
        
        analysis += "\n**ë°±ì—… ìœˆë„ìš°**:\n"
        for cluster in clusters:
            cluster_id = cluster.get('db_cluster_identifier', 'N/A')
            backup_window = cluster.get('preferred_backup_window', 'N/A')
            maintenance_window = cluster.get('preferred_maintenance_window', 'N/A')
            analysis += f"- {cluster_id}: {backup_window} (ìœ ì§€ë³´ìˆ˜: {maintenance_window})\n"
        
        analysis += "\n"
        return analysis

    def analyze_elasticache(self) -> str:
        """ElastiCache ìƒì„¸ ë¶„ì„"""
        clusters_data = self.load_json_data(self.data_files['elasticache_clusters'])
        replication_data = self.load_json_data(self.data_files['elasticache_replication_groups'])
        
        analysis = """---

## âš¡ Amazon ElastiCache ìƒì„¸ ë¶„ì„

### ElastiCache í´ëŸ¬ìŠ¤í„° í˜„í™©
"""
        
        if not clusters_data:
            analysis += "âŒ ElastiCache í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
        else:
            clusters = clusters_data['rows']
            total_count = len(clusters)
            available_count = len([c for c in clusters if c.get('cache_cluster_status') == 'available'])
            auto_upgrade_count = len([c for c in clusters if c.get('auto_minor_version_upgrade') == True])
            
            # ì—”ì§„ë³„ í†µê³„
            engine_stats = {}
            for cluster in clusters:
                engine = cluster.get('engine', 'Unknown')
                if engine not in engine_stats:
                    engine_stats[engine] = {
                        'count': 0,
                        'total_nodes': 0,
                        'node_types': set()
                    }
                engine_stats[engine]['count'] += 1
                engine_stats[engine]['total_nodes'] += cluster.get('num_cache_nodes', 0)
                engine_stats[engine]['node_types'].add(cluster.get('cache_node_type', 'N/A'))
            
            analysis += f"""
**ğŸ“ˆ ElastiCache í´ëŸ¬ìŠ¤í„° í†µê³„**
- **ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜**: {total_count}ê°œ
- **ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„°**: {available_count}ê°œ
- **ê°€ìš©ì„±**: {self.calculate_percentage(available_count, total_count)}%

#### ğŸ“‹ ElastiCache í´ëŸ¬ìŠ¤í„° ìƒì„¸ ëª©ë¡

| í´ëŸ¬ìŠ¤í„° ID | ì—”ì§„ | ë²„ì „ | ë…¸ë“œ íƒ€ì… | ìƒíƒœ | ë…¸ë“œ ìˆ˜ | AZ | ë³µì œ ê·¸ë£¹ |
|-------------|------|------|-----------|------|---------|----|-----------| 
"""
            
            # í´ëŸ¬ìŠ¤í„° ìƒì„¸ ëª©ë¡
            for cluster in clusters:
                cluster_id = cluster.get('cache_cluster_id', 'N/A')
                engine = cluster.get('engine', 'N/A')
                version = cluster.get('engine_version', 'N/A')
                node_type = cluster.get('cache_node_type', 'N/A')
                status = cluster.get('cache_cluster_status', 'N/A')
                num_nodes = cluster.get('num_cache_nodes', 0)
                az = cluster.get('preferred_availability_zone', 'N/A')
                repl_group = cluster.get('replication_group_id', 'ì—†ìŒ')
                
                analysis += f"| {cluster_id} | {engine} | {version} | {node_type} | {status} | {num_nodes} | {az} | {repl_group} |\n"
            
            # ì—”ì§„ë³„ ë¶„í¬
            analysis += f"""
#### ğŸ”§ ì—”ì§„ë³„ ë¶„í¬

| ì—”ì§„ | í´ëŸ¬ìŠ¤í„° ìˆ˜ | í‰ê·  ë…¸ë“œ ìˆ˜ | ì£¼ìš” ë…¸ë“œ íƒ€ì… |
|------|-------------|--------------|----------------|
"""
            
            for engine, stats in engine_stats.items():
                avg_nodes = stats['total_nodes'] / stats['count'] if stats['count'] > 0 else 0
                main_node_type = list(stats['node_types'])[0] if stats['node_types'] else 'N/A'
                analysis += f"| {engine} | {stats['count']}ê°œ | {avg_nodes:.1f}ê°œ | {main_node_type} |\n"
            
            # ìœ ì§€ë³´ìˆ˜ ì„¤ì •
            analysis += f"""
#### âš™ï¸ ìœ ì§€ë³´ìˆ˜ ë° ì—…ê·¸ë ˆì´ë“œ ì„¤ì •

**ìë™ ë§ˆì´ë„ˆ ë²„ì „ ì—…ê·¸ë ˆì´ë“œ**:
- í™œì„±í™”ëœ í´ëŸ¬ìŠ¤í„°: {auto_upgrade_count}/{total_count}ê°œ
- ê¶Œì¥ì‚¬í•­: {'âœ… ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì—ì„œ í™œì„±í™”ë¨' if auto_upgrade_count == total_count else 'âš ï¸ ì¼ë¶€ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë¹„í™œì„±í™”ë¨'}

**ìœ ì§€ë³´ìˆ˜ ìœˆë„ìš°**:
"""
            
            for cluster in clusters:
                cluster_id = cluster.get('cache_cluster_id', 'N/A')
                maintenance_window = cluster.get('preferred_maintenance_window', 'N/A')
                analysis += f"- {cluster_id}: {maintenance_window}\n"
        
        # ë³µì œ ê·¸ë£¹ ë¶„ì„
        analysis += "\n### ElastiCache ë³µì œ ê·¸ë£¹ ë¶„ì„\n"
        
        if not replication_data:
            analysis += "âŒ ElastiCache ë³µì œ ê·¸ë£¹ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
        else:
            repl_groups = replication_data['rows']
            total_repl = len(repl_groups)
            available_repl = len([r for r in repl_groups if r.get('status') == 'available'])
            transit_encrypted = len([r for r in repl_groups if r.get('transit_encryption_enabled') == True])
            rest_encrypted = len([r for r in repl_groups if r.get('at_rest_encryption_enabled') == True])
            
            analysis += f"""
**ğŸ“ˆ ë³µì œ ê·¸ë£¹ í†µê³„**
- **ì´ ë³µì œ ê·¸ë£¹**: {total_repl}ê°œ
- **ì‚¬ìš© ê°€ëŠ¥í•œ ê·¸ë£¹**: {available_repl}ê°œ

#### ğŸ“‹ ë³µì œ ê·¸ë£¹ ìƒì„¸ ëª©ë¡

| ë³µì œ ê·¸ë£¹ ID | ì„¤ëª… | ìƒíƒœ | ë…¸ë“œ íƒ€ì… | ë©¤ë²„ ìˆ˜ | ìë™ ì¥ì• ì¡°ì¹˜ | Multi-AZ |
|--------------|------|------|-----------|---------|---------------|----------|
"""
            
            for group in repl_groups:
                group_id = group.get('replication_group_id', 'N/A')
                description = group.get('description', 'N/A')
                status = group.get('status', 'N/A')
                node_type = group.get('cache_node_type', 'N/A')
                member_count = len(group.get('member_clusters', []))
                auto_failover = "âœ…" if group.get('automatic_failover') == 'enabled' else "âŒ"
                multi_az = "âœ…" if group.get('multi_az') == 'enabled' else "âŒ"
                
                analysis += f"| {group_id} | {description} | {status} | {node_type} | {member_count} | {auto_failover} | {multi_az} |\n"
            
            # ë³´ì•ˆ ë° ì•”í˜¸í™” ì„¤ì •
            analysis += f"""
#### ğŸ” ë³´ì•ˆ ë° ì•”í˜¸í™” ì„¤ì •

**ì „ì†¡ ì¤‘ ì•”í˜¸í™”**:
- í™œì„±í™”ëœ ê·¸ë£¹: {transit_encrypted}/{total_repl}ê°œ

**ì €ì¥ ì‹œ ì•”í˜¸í™”**:
- í™œì„±í™”ëœ ê·¸ë£¹: {rest_encrypted}/{total_repl}ê°œ

"""
        
        return analysis

    def analyze_opensearch(self) -> str:
        """OpenSearch ìƒì„¸ ë¶„ì„"""
        opensearch_data = self.load_json_data(self.data_files['opensearch_domains'])
        
        analysis = """---

## ğŸ” Amazon OpenSearch ìƒì„¸ ë¶„ì„

### OpenSearch ë„ë©”ì¸ í˜„í™©
"""
        
        if not opensearch_data:
            analysis += "âŒ OpenSearch ë„ë©”ì¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            return analysis
        
        domains = opensearch_data['rows']
        total_count = len(domains)
        processing_count = len([d for d in domains if d.get('processing') == True])
        
        analysis += f"""
**ğŸ“ˆ OpenSearch ë„ë©”ì¸ í†µê³„**
- **ì´ ë„ë©”ì¸ ìˆ˜**: {total_count}ê°œ
- **ì²˜ë¦¬ ì¤‘ì¸ ë„ë©”ì¸**: {processing_count}ê°œ

#### ğŸ“‹ OpenSearch ë„ë©”ì¸ ìƒì„¸ ëª©ë¡

| ë„ë©”ì¸ëª… | ì—”ì§„ ë²„ì „ | ì—”ë“œí¬ì¸íŠ¸ | ì²˜ë¦¬ ìƒíƒœ | ìƒì„±ì¼ | ì‚­ì œì¼ |
|----------|-----------|------------|-----------|--------|--------|
"""
        
        # ë„ë©”ì¸ ìƒì„¸ ëª©ë¡
        for domain in domains:
            domain_name = domain.get('domain_name', 'N/A')
            engine_version = domain.get('engine_version', 'N/A')
            endpoint = domain.get('endpoint', 'N/A')
            processing_status = "ğŸ”„ ì²˜ë¦¬ì¤‘" if domain.get('processing') else "âœ… ì™„ë£Œ"
            created = str(domain.get('created', 'N/A'))
            deleted = str(domain.get('deleted', 'N/A'))
            
            analysis += f"| {domain_name} | {engine_version} | {endpoint} | {processing_status} | {created} | {deleted} |\n"
        
        # í´ëŸ¬ìŠ¤í„° êµ¬ì„± ë¶„ì„
        analysis += f"""
#### âš™ï¸ í´ëŸ¬ìŠ¤í„° êµ¬ì„± ë¶„ì„

**ì¸ìŠ¤í„´ìŠ¤ êµ¬ì„±**:
"""
        
        for domain in domains:
            domain_name = domain.get('domain_name', 'N/A')
            cluster_config = domain.get('cluster_config', {})
            instance_type = cluster_config.get('instance_type', 'N/A')
            instance_count = cluster_config.get('instance_count', 0)
            analysis += f"- **{domain_name}**: {instance_type} ({instance_count}ê°œ ì¸ìŠ¤í„´ìŠ¤)\n"
        
        # ë³´ì•ˆ ë° ë„¤íŠ¸ì›Œí¬ ì„¤ì •
        analysis += f"""
#### ğŸ” ë³´ì•ˆ ë° ë„¤íŠ¸ì›Œí¬ ì„¤ì •

**VPC êµ¬ì„±**:
"""
        
        for domain in domains:
            domain_name = domain.get('domain_name', 'N/A')
            vpc_options = domain.get('vpc_options')
            if vpc_options and vpc_options.get('subnet_ids'):
                subnet_count = len(vpc_options.get('subnet_ids', []))
                analysis += f"- **{domain_name}**: VPC ë‚´ë¶€ ë°°ì¹˜ (ì„œë¸Œë„·: {subnet_count}ê°œ)\n"
            else:
                analysis += f"- **{domain_name}**: í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤\n"
        
        analysis += f"""
**ì €ì¥ ì‹œ ì•”í˜¸í™”**:
"""
        
        for domain in domains:
            domain_name = domain.get('domain_name', 'N/A')
            encryption_options = domain.get('encryption_at_rest_options', {})
            encrypted = "ğŸ”’ í™œì„±í™”" if encryption_options.get('enabled') else "ğŸ”“ ë¹„í™œì„±í™”"
            analysis += f"- **{domain_name}**: {encrypted}\n"
        
        analysis += "\n"
        return analysis

    def analyze_athena(self) -> str:
        """Athena ìƒì„¸ ë¶„ì„"""
        athena_data = self.load_json_data(self.data_files['athena_workgroups'])
        
        analysis = """---

## ğŸ“Š Amazon Athena ë¶„ì„

### Athena ì›Œí¬ê·¸ë£¹ í˜„í™©
"""
        
        if not athena_data:
            analysis += "âŒ Athena ì›Œí¬ê·¸ë£¹ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
            return analysis
        
        workgroups = athena_data['rows']
        total_count = len(workgroups)
        enabled_count = len([w for w in workgroups if w.get('state') == 'ENABLED'])
        
        analysis += f"""
**ğŸ“ˆ Athena ì›Œí¬ê·¸ë£¹ í†µê³„**
- **ì´ ì›Œí¬ê·¸ë£¹ ìˆ˜**: {total_count}ê°œ
- **í™œì„±í™”ëœ ì›Œí¬ê·¸ë£¹**: {enabled_count}ê°œ

#### ğŸ“‹ Athena ì›Œí¬ê·¸ë£¹ ìƒì„¸ ëª©ë¡

| ì›Œí¬ê·¸ë£¹ëª… | ì„¤ëª… | ìƒíƒœ | ìƒì„±ì¼ | ì¶œë ¥ ìœ„ì¹˜ | ì•”í˜¸í™” |
|------------|------|------|--------|-----------|--------|
"""
        
        # ì›Œí¬ê·¸ë£¹ ìƒì„¸ ëª©ë¡
        for workgroup in workgroups:
            name = workgroup.get('name', 'N/A')
            description = workgroup.get('description', 'N/A')
            state = workgroup.get('state', 'N/A')
            creation_time = workgroup.get('creation_time', 'N/A')
            output_location = workgroup.get('output_location', 'N/A')
            encryption_option = workgroup.get('encryption_option', 'ì—†ìŒ')
            
            analysis += f"| {name} | {description} | {state} | {creation_time} | {output_location} | {encryption_option} |\n"
        
        analysis += "\n"
        return analysis
    def generate_recommendations(self) -> str:
        """ì¢…í•© ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        rds_data = self.load_json_data(self.data_files['rds_instances'])
        replication_data = self.load_json_data(self.data_files['elasticache_replication_groups'])
        
        recommendations = """---

## ğŸ“‹ ì¢…í•© ê¶Œì¥ì‚¬í•­ ë° ê°œì„  ê³„íš

### ğŸ”´ ë†’ì€ ìš°ì„ ìˆœìœ„ (ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”)

#### ë³´ì•ˆ ê°•í™”
"""
        
        # RDS ë³´ì•ˆ ê¶Œì¥ì‚¬í•­
        if rds_data:
            instances = rds_data['rows']
            unencrypted_rds = len([i for i in instances if not i.get('storage_encrypted')])
            public_rds = len([i for i in instances if i.get('publicly_accessible')])
            
            if unencrypted_rds > 0:
                recommendations += f"1. **RDS ì•”í˜¸í™” ë¯¸ì ìš©**: {unencrypted_rds}ê°œ ì¸ìŠ¤í„´ìŠ¤ì— ì €ì¥ ì‹œ ì•”í˜¸í™” ì ìš© í•„ìš”\n"
            
            if public_rds > 0:
                recommendations += f"2. **RDS ê³µê°œ ì ‘ê·¼**: {public_rds}ê°œ ì¸ìŠ¤í„´ìŠ¤ì˜ ê³µê°œ ì ‘ê·¼ ì„¤ì • ê²€í†  ë° ì œí•œ í•„ìš”\n"
        
        # ElastiCache ë³´ì•ˆ ê¶Œì¥ì‚¬í•­
        if replication_data:
            groups = replication_data['rows']
            unencrypted_cache = len([g for g in groups if not g.get('at_rest_encryption_enabled') or not g.get('transit_encryption_enabled')])
            
            if unencrypted_cache > 0:
                recommendations += f"3. **ElastiCache ì•”í˜¸í™”**: {unencrypted_cache}ê°œ ë³µì œ ê·¸ë£¹ì— ì „ì†¡/ì €ì¥ ì•”í˜¸í™” ì ìš© í•„ìš”\n"
        
        recommendations += """
#### ê³ ê°€ìš©ì„± êµ¬ì„±
"""
        
        # RDS Multi-AZ ê¶Œì¥ì‚¬í•­
        if rds_data:
            instances = rds_data['rows']
            non_multi_az = len([i for i in instances if not i.get('multi_az')])
            
            if non_multi_az > 0:
                recommendations += f"1. **Multi-AZ ë¯¸êµ¬ì„±**: {non_multi_az}ê°œ RDS ì¸ìŠ¤í„´ìŠ¤ì— Multi-AZ êµ¬ì„± ê²€í† \n"
        
        # ElastiCache ìë™ ì¥ì• ì¡°ì¹˜ ê¶Œì¥ì‚¬í•­
        if replication_data:
            groups = replication_data['rows']
            non_failover_cache = len([g for g in groups if g.get('automatic_failover') != 'enabled'])
            
            if non_failover_cache > 0:
                recommendations += f"2. **ElastiCache ìë™ ì¥ì• ì¡°ì¹˜**: {non_failover_cache}ê°œ ë³µì œ ê·¸ë£¹ì— ìë™ ì¥ì• ì¡°ì¹˜ í™œì„±í™” í•„ìš”\n"
        
        recommendations += """
### ğŸŸ¡ ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (1-3ê°œì›” ë‚´ ì¡°ì¹˜)

#### ì„±ëŠ¥ ìµœì í™”
1. **Performance Insights í™œì„±í™”**: RDS ì¸ìŠ¤í„´ìŠ¤ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê°•í™”
2. **ElastiCache ë…¸ë“œ íƒ€ì… ìµœì í™”**: ì›Œí¬ë¡œë“œì— ë§ëŠ” ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ê²€í† 
3. **OpenSearch í´ëŸ¬ìŠ¤í„° í¬ê¸° ì¡°ì •**: ì‚¬ìš© íŒ¨í„´ ê¸°ë°˜ ìµœì í™”

#### ë°±ì—… ë° ë³µêµ¬
1. **ë°±ì—… ë³´ì¡´ ê¸°ê°„ í‘œì¤€í™”**: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë°±ì—… ì •ì±… ìˆ˜ë¦½
2. **Point-in-Time Recovery í…ŒìŠ¤íŠ¸**: ì •ê¸°ì ì¸ ë³µêµ¬ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
3. **Cross-Region ë°±ì—…**: ì¬í•´ ë³µêµ¬ë¥¼ ìœ„í•œ ë‹¤ì¤‘ ë¦¬ì „ ë°±ì—… ê³ ë ¤

### ğŸŸ¢ ë‚®ì€ ìš°ì„ ìˆœìœ„ (ì¥ê¸° ê³„íš)

#### ë¹„ìš© ìµœì í™”
1. **Reserved Instance í™œìš©**: ì¥ê¸° ì‹¤í–‰ ë°ì´í„°ë² ì´ìŠ¤ ë¹„ìš© ì ˆê°
2. **Aurora Serverless ê²€í† **: ê°€ë³€ ì›Œí¬ë¡œë“œì— ëŒ€í•œ ì„œë²„ë¦¬ìŠ¤ ì˜µì…˜ í‰ê°€
3. **ìŠ¤í† ë¦¬ì§€ íƒ€ì… ìµœì í™”**: gp3 ìŠ¤í† ë¦¬ì§€ í™œìš© ê²€í† 

#### í˜„ëŒ€í™” ë° ë§ˆì´ê·¸ë ˆì´ì…˜
1. **Aurora ë§ˆì´ê·¸ë ˆì´ì…˜**: ê¸°ì¡´ RDSë¥¼ Auroraë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²€í† 
2. **DynamoDB í™œìš©**: NoSQL ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ DynamoDB ë„ì… ê²€í† 
3. **OpenSearch ìµœì‹  ë²„ì „**: ì—”ì§„ ë²„ì „ ì—…ê·¸ë ˆì´ë“œ ê³„íš

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •

### ê¶Œì¥ CloudWatch ë©”íŠ¸ë¦­
1. **RDS**: CPU ì‚¬ìš©ë¥ , ì—°ê²° ìˆ˜, ì½ê¸°/ì“°ê¸° ì§€ì—°ì‹œê°„
2. **ElastiCache**: CPU ì‚¬ìš©ë¥ , ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ , ìºì‹œ íˆíŠ¸ìœ¨
3. **OpenSearch**: í´ëŸ¬ìŠ¤í„° ìƒíƒœ, ê²€ìƒ‰ ì§€ì—°ì‹œê°„, ì¸ë±ì‹± ì†ë„

### ì•Œë¦¼ ì„ê³„ê°’ ê¶Œì¥ì‚¬í•­
- **RDS CPU**: 80% ì´ìƒ ì§€ì† ì‹œ ì•Œë¦¼
- **ElastiCache ë©”ëª¨ë¦¬**: 90% ì´ìƒ ì‚¬ìš© ì‹œ ì•Œë¦¼
- **OpenSearch í´ëŸ¬ìŠ¤í„°**: Yellow/Red ìƒíƒœ ì‹œ ì¦‰ì‹œ ì•Œë¦¼

---

## ğŸ’° ì˜ˆìƒ ë¹„ìš© ë¶„ì„

### ì›”ê°„ ì˜ˆìƒ ë¹„ìš© (ì¶”ì •)
- **RDS ì¸ìŠ¤í„´ìŠ¤**: ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë° ìŠ¤í† ë¦¬ì§€ ê¸°ë°˜ ë¹„ìš© ê³„ì‚° í•„ìš”
- **ElastiCache**: ë…¸ë“œ íƒ€ì… ë° ê°œìˆ˜ ê¸°ë°˜ ë¹„ìš© ê³„ì‚° í•„ìš”
- **OpenSearch**: ì¸ìŠ¤í„´ìŠ¤ íƒ€ì… ë° ìŠ¤í† ë¦¬ì§€ ê¸°ë°˜ ë¹„ìš© ê³„ì‚° í•„ìš”

### ë¹„ìš© ìµœì í™” ê¸°íšŒ
1. **ì¸ìŠ¤í„´ìŠ¤ í¬ê¸° ì¡°ì •**: ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ í›„ ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •
2. **ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤**: 1ë…„ ë˜ëŠ” 3ë…„ ì•½ì •ìœ¼ë¡œ ìµœëŒ€ 75% ë¹„ìš© ì ˆê°
3. **ìŠ¤í† ë¦¬ì§€ ìµœì í™”**: ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ë° ìŠ¤í† ë¦¬ì§€ íƒ€ì… ìµœì í™”

---

*ğŸ“… ë¶„ì„ ì™„ë£Œ ì‹œê°„: {self.current_time}*  
*ğŸ”„ ë‹¤ìŒ ë¶„ì„ ê¶Œì¥ ì£¼ê¸°: ì›” 1íšŒ*

---
"""
        return recommendations

    def generate_report(self) -> str:
        """ì „ì²´ ë³´ê³ ì„œ ìƒì„±"""
        self.logger.info("ğŸ—„ï¸ Enhanced Database Analysis ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
        
        # ê° ì„¹ì…˜ ìƒì„±
        executive_summary = self.generate_executive_summary()
        rds_analysis = self.analyze_rds_instances()
        rds_clusters = self.analyze_rds_clusters()
        elasticache_analysis = self.analyze_elasticache()
        opensearch_analysis = self.analyze_opensearch()
        athena_analysis = self.analyze_athena()
        recommendations = self.generate_recommendations()
        
        # ì „ì²´ ë³´ê³ ì„œ ì¡°í•©
        full_report = (
            executive_summary +
            rds_analysis +
            rds_clusters +
            elasticache_analysis +
            opensearch_analysis +
            athena_analysis +
            recommendations
        )
        
        return full_report

    def save_report(self, content: str, filename: str = "05-database-analysis.md") -> None:
        """ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        output_path = self.report_dir / filename
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            self.logger.info(f"âœ… Enhanced Database Analysis ìƒì„± ì™„ë£Œ: {filename}")
            
            # íŒŒì¼ í¬ê¸° ì •ë³´
            file_size = output_path.stat().st_size
            self.logger.info(f"ğŸ“„ ë³´ê³ ì„œ í¬ê¸°: {file_size:,} bytes")
            
        except Exception as e:
            self.logger.error(f"âŒ ë³´ê³ ì„œ ì €ì¥ ì‹¤íŒ¨: {e}")
            sys.exit(1)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ë³´ê³ ì„œ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = DatabaseReportGenerator()
        
        # ë³´ê³ ì„œ ìƒì„±
        report_content = generator.generate_report()
        
        # ë³´ê³ ì„œ ì €ì¥
        generator.save_report(report_content)
        
        print("ğŸ‰ Enhanced Database Analysis ë³´ê³ ì„œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    except KeyboardInterrupt:
        print("\nâŒ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
