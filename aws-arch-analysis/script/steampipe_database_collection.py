#!/usr/bin/env python3
"""
ì™„ì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (RDS, NoSQL, ë¶„ì„ ì„œë¹„ìŠ¤ í¬í•¨)
Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ê¸°ëŠ¥ ë° ì¶œë ¥ í˜•ì‹ ì œê³µ
"""

import os
import subprocess
import glob
from pathlib import Path
from typing import List, Tuple

class SteampipeDatabaseCollector:
    def __init__(self, region: str = "ap-northeast-2", report_dir: str = None):
        # ìŠ¤í¬ë¦½íŠ¸ì˜ ì‹¤ì œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²½ë¡œ ì„¤ì •
        if report_dir is None:
            script_dir = Path(__file__).parent
            project_root = script_dir.parent.parent
            report_dir = str(project_root / "aws-arch-analysis" / "report")
        self.region = region
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        self.log_file = self.report_dir / "steampipe_database_collection.log"
        self.error_log = self.report_dir / "steampipe_database_errors.log"
        
        # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
        self.log_file.write_text("")
        self.error_log.write_text("")
        
        self.success_count = 0
        self.total_count = 0
        
        # ìƒ‰ìƒ ì½”ë“œ
        self.RED = '\033[0;31m'
        self.GREEN = '\033[0;32m'
        self.YELLOW = '\033[1;33m'
        self.BLUE = '\033[0;34m'
        self.PURPLE = '\033[0;35m'
        self.CYAN = '\033[0;36m'
        self.NC = '\033[0m'  # No Color

    def log_info(self, message: str):
        print(f"{self.BLUE}[INFO]{self.NC} {message}")
        with open(self.log_file, 'a') as f:
            f.write(f"[INFO] {message}\n")

    def log_success(self, message: str):
        print(f"{self.GREEN}[SUCCESS]{self.NC} {message}")
        with open(self.log_file, 'a') as f:
            f.write(f"[SUCCESS] {message}\n")

    def log_warning(self, message: str):
        print(f"{self.YELLOW}[WARNING]{self.NC} {message}")
        with open(self.log_file, 'a') as f:
            f.write(f"[WARNING] {message}\n")

    def log_error(self, message: str):
        print(f"{self.RED}[ERROR]{self.NC} {message}")
        with open(self.error_log, 'a') as f:
            f.write(f"[ERROR] {message}\n")

    def log_rds(self, message: str):
        print(f"{self.PURPLE}[RDS]{self.NC} {message}")
        with open(self.log_file, 'a') as f:
            f.write(f"[RDS] {message}\n")

    def log_nosql(self, message: str):
        print(f"{self.CYAN}[NoSQL]{self.NC} {message}")
        with open(self.log_file, 'a') as f:
            f.write(f"[NoSQL] {message}\n")

    def check_steampipe_plugin(self):
        """Steampipe AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸"""
        self.log_info("Steampipe AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘...")
        try:
            result = subprocess.run(
                ["steampipe", "plugin", "list"],
                capture_output=True,
                text=True,
                check=True
            )
            if "aws" not in result.stdout:
                self.log_warning("AWS í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘...")
                subprocess.run(["steampipe", "plugin", "install", "aws"], check=True)
        except subprocess.CalledProcessError:
            self.log_warning("Steampipe í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

    def execute_steampipe_query(self, description: str, query: str, output_file: str) -> bool:
        self.log_info(f"ìˆ˜ì§‘ ì¤‘: {description}")
        self.total_count += 1
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ report_dirë¡œ ë³€ê²½
            os.chdir(self.report_dir)
            
            result = subprocess.run(
                ["steampipe", "query", query, "--output", "json"],
                capture_output=True,
                text=True,
                check=True
            )
            
            output_path = self.report_dir / output_file
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(result.stdout)
            
            file_size = output_path.stat().st_size
            if file_size > 50:
                self.log_success(f"{description} ì™„ë£Œ ({output_file}, {file_size} bytes)")
                self.success_count += 1
                return True
            else:
                self.log_warning(f"{description} - ë°ì´í„° ì—†ìŒ ({output_file}, {file_size} bytes)")
                return False
                
        except subprocess.CalledProcessError as e:
            # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ error_logì— ê¸°ë¡
            error_msg = f"{description} ì‹¤íŒ¨ - {output_file}"
            if e.stderr:
                error_msg += f": {e.stderr.strip()}"
            self.log_error(error_msg)
            
            # ì˜¤ë¥˜ ë¡œê·¸ì— ì¶”ê°€ ì •ë³´ ê¸°ë¡
            with open(self.error_log, 'a') as f:
                f.write(f"\nQuery failed: {query}\n")
                f.write(f"Error: {e.stderr}\n")
            
            return False

    def get_rds_queries(self) -> List[Tuple[str, str, str]]:
        """RDS ê¸°ë³¸ ë¦¬ì†ŒìŠ¤ ì¿¼ë¦¬ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€)"""
        return [
            (
                "RDS DB ì¸ìŠ¤í„´ìŠ¤",
                f"select db_instance_identifier, class, engine, engine_version, status, allocated_storage, storage_type, storage_encrypted, multi_az, publicly_accessible, vpc_security_groups, db_subnet_group_name, backup_retention_period, preferred_backup_window, preferred_maintenance_window, auto_minor_version_upgrade, deletion_protection, tags from aws_rds_db_instance where region = '{self.region}'",
                "database_rds_instances.json"
            ),
            (
                "RDS DB í´ëŸ¬ìŠ¤í„°",
                f"select db_cluster_identifier, engine, engine_version, database_name, members, vpc_security_groups, db_subnet_group, backup_retention_period, preferred_backup_window, preferred_maintenance_window, multi_az, storage_encrypted, kms_key_id, endpoint, reader_endpoint, status, deletion_protection, tags from aws_rds_db_cluster where region = '{self.region}'",
                "database_rds_clusters.json"
            ),
            (
                "RDS ì„œë¸Œë„· ê·¸ë£¹",
                f"select name, description, vpc_id, status, subnets, tags from aws_rds_db_subnet_group where region = '{self.region}'",
                "database_rds_subnet_groups.json"
            ),
            (
                "RDS íŒŒë¼ë¯¸í„° ê·¸ë£¹",
                f"select name, description, db_parameter_group_family, tags from aws_rds_db_parameter_group where region = '{self.region}'",
                "database_rds_parameter_groups.json"
            ),
            (
                "RDS ìŠ¤ëƒ…ìƒ·",
                f"select db_snapshot_identifier, db_instance_identifier, create_time, engine, allocated_storage, status, encrypted, kms_key_id, tags from aws_rds_db_snapshot where region = '{self.region}'",
                "database_rds_snapshots.json"
            )
        ]

    def get_dynamodb_queries(self) -> List[Tuple[str, str, str]]:
        """DynamoDB ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¿¼ë¦¬ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€)"""
        return [
            (
                "DynamoDB í…Œì´ë¸”",
                f"select name, table_status, creation_date_time, billing_mode, read_capacity, write_capacity, stream_specification, deletion_protection_enabled, table_size_bytes, item_count, tags from aws_dynamodb_table where region = '{self.region}'",
                "database_dynamodb_tables.json"
            ),
            (
                "DynamoDB ë°±ì—…",
                f"select name, table_name, backup_status, backup_type, backup_creation_datetime, backup_size_bytes from aws_dynamodb_backup where region = '{self.region}'",
                "database_dynamodb_backups.json"
            )
        ]

    def get_elasticache_queries(self) -> List[Tuple[str, str, str]]:
        """ElastiCache ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¿¼ë¦¬ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€)"""
        return [
            (
                "ElastiCache í´ëŸ¬ìŠ¤í„°",
                f"select cache_cluster_id, cache_node_type, engine, engine_version, cache_cluster_status, num_cache_nodes, preferred_availability_zone, cache_cluster_create_time, preferred_maintenance_window, auto_minor_version_upgrade, security_groups, replication_group_id from aws_elasticache_cluster where region = '{self.region}'",
                "database_elasticache_clusters.json"
            ),
            (
                "ElastiCache ë³µì œ ê·¸ë£¹",
                f"select replication_group_id, description, status, member_clusters, automatic_failover, multi_az, cache_node_type, auth_token_enabled, transit_encryption_enabled, at_rest_encryption_enabled from aws_elasticache_replication_group where region = '{self.region}'",
                "database_elasticache_replication_groups.json"
            )
        ]

    def get_warehouse_queries(self) -> List[Tuple[str, str, str]]:
        """ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¿¼ë¦¬ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€)"""
        return [
            (
                "Redshift í´ëŸ¬ìŠ¤í„°",
                f"select cluster_identifier, node_type, cluster_status, master_username, db_name, endpoint, cluster_create_time, number_of_nodes, publicly_accessible, encrypted, vpc_id, tags from aws_redshift_cluster where region = '{self.region}'",
                "database_redshift_clusters.json"
            ),
            (
                "OpenSearch ë„ë©”ì¸",
                f"select domain_name, engine_version, endpoint, processing, created, deleted, cluster_config, ebs_options, vpc_options, encryption_at_rest_options, tags from aws_opensearch_domain where region = '{self.region}'",
                "database_opensearch_domains.json"
            )
        ]

    def get_bigdata_queries(self) -> List[Tuple[str, str, str]]:
        """ë¹…ë°ì´í„° ì²˜ë¦¬ ê´€ë ¨ ë¦¬ì†ŒìŠ¤ ì¿¼ë¦¬ (ì‹¤ì œ ìŠ¤í‚¤ë§ˆ ê¸°ì¤€)"""
        return [
            (
                "EMR í´ëŸ¬ìŠ¤í„°",
                f"select id, name, status, ec2_instance_attributes, log_uri, release_label, auto_terminate, termination_protected, applications, service_role, tags from aws_emr_cluster where region = '{self.region}'",
                "database_emr_clusters.json"
            ),
            (
                "Kinesis ìŠ¤íŠ¸ë¦¼",
                f"select stream_name, stream_status, retention_period_hours, open_shard_count, stream_creation_timestamp, encryption_type, tags from aws_kinesis_stream where region = '{self.region}'",
                "database_kinesis_streams.json"
            ),
            (
                "Glue ë°ì´í„°ë² ì´ìŠ¤",
                f"select name, description, location_uri, create_time from aws_glue_catalog_database where region = '{self.region}'",
                "database_glue_databases.json"
            ),
            (
                "Athena ì›Œí¬ê·¸ë£¹",
                f"select name, description, state, creation_time, output_location, encryption_option from aws_athena_workgroup where region = '{self.region}'",
                "database_athena_workgroups.json"
            )
        ]

    def collect_data(self):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        self.log_info("ğŸ—„ï¸ ì™„ì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (RDS, NoSQL, ë¶„ì„ ì„œë¹„ìŠ¤ í¬í•¨)")
        self.log_info(f"Region: {self.region}")
        self.log_info(f"Report Directory: {self.report_dir}")
        
        # Steampipe í”ŒëŸ¬ê·¸ì¸ í™•ì¸
        self.check_steampipe_plugin()
        
        # RDS ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
        self.log_rds("ğŸ›ï¸ RDS ì¸ìŠ¤í„´ìŠ¤ ë° í´ëŸ¬ìŠ¤í„° ìˆ˜ì§‘ ì‹œì‘...")
        rds_queries = self.get_rds_queries()
        for description, query, output_file in rds_queries:
            self.execute_steampipe_query(description, query, output_file)
        
        # DynamoDB ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
        self.log_nosql("ğŸ”¥ DynamoDB ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        dynamodb_queries = self.get_dynamodb_queries()
        for description, query, output_file in dynamodb_queries:
            self.execute_steampipe_query(description, query, output_file)
        
        # ElastiCache ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘
        self.log_nosql("âš¡ ElastiCache ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        elasticache_queries = self.get_elasticache_queries()
        for description, query, output_file in elasticache_queries:
            self.execute_steampipe_query(description, query, output_file)
        
        # ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„œë¹„ìŠ¤ ìˆ˜ì§‘
        self.log_info("ğŸ¢ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ì„œë¹„ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        warehouse_queries = self.get_warehouse_queries()
        for description, query, output_file in warehouse_queries:
            self.execute_steampipe_query(description, query, output_file)
        
        # ë¹…ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤ ìˆ˜ì§‘
        self.log_info("ğŸš€ ë¹…ë°ì´í„° ì²˜ë¦¬ ì„œë¹„ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        bigdata_queries = self.get_bigdata_queries()
        for description, query, output_file in bigdata_queries:
            self.execute_steampipe_query(description, query, output_file)
        
        # ê²°ê³¼ ìš”ì•½
        self.log_success("ì™„ì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        self.log_info(f"ì„±ê³µ: {self.success_count}/{self.total_count}")
        
        # Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ì¶œë ¥ í˜•ì‹
        self.display_file_list()
        self.display_statistics()
        self.display_error_summary()
        self.display_next_steps()
        
        self.log_info("ğŸ‰ ì™„ì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

    def display_file_list(self):
        """ìƒì„±ëœ íŒŒì¼ ëª©ë¡ í‘œì‹œ (Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í˜•ì‹)"""
        print(f"\n{self.BLUE}ğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:{self.NC}")
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ íŒŒì¼ë“¤ ê²€ìƒ‰
        files_found = list(self.report_dir.glob("database_*.json"))
        files_found.sort()
        
        for file_path in files_found:
            file_size = file_path.stat().st_size
            if file_size > 100:
                print(f"{self.GREEN}âœ“ {file_path.name} ({file_size} bytes){self.NC}")
            else:
                print(f"{self.YELLOW}âš  {file_path.name} ({file_size} bytes) - ë°ì´í„° ì—†ìŒ{self.NC}")

    def display_statistics(self):
        """ìˆ˜ì§‘ í†µê³„ í‘œì‹œ (Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ í˜•ì‹)"""
        print(f"\n{self.BLUE}ğŸ“Š ìˆ˜ì§‘ í†µê³„:{self.NC}")
        print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {self.total_count}")
        print(f"ì„±ê³µí•œ ì¿¼ë¦¬: {self.success_count}")
        print(f"ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {self.total_count - self.success_count}")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©
        print(f"\n{self.BLUE}ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:{self.NC}")
        print("ğŸ›ï¸  RDS ë¦¬ì†ŒìŠ¤: 5ê°œ")
        print("ğŸ”¥ DynamoDB: 2ê°œ")
        print("âš¡ ElastiCache: 2ê°œ")
        print("ğŸ¢ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤: 2ê°œ")
        print("ğŸš€ ë¹…ë°ì´í„° ì²˜ë¦¬: 4ê°œ")
        print("ğŸ“Š ì´ ë¦¬ì†ŒìŠ¤ íƒ€ì…: 15ê°œ")

    def display_error_summary(self):
        """ì˜¤ë¥˜ ìš”ì•½ í‘œì‹œ"""
        if self.error_log.exists() and self.error_log.stat().st_size > 0:
            self.log_warning(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. {self.error_log.name} íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
            
            print(f"\n{self.YELLOW}ìµœê·¼ ì˜¤ë¥˜ (ë§ˆì§€ë§‰ 5ì¤„):{self.NC}")
            try:
                with open(self.error_log, 'r') as f:
                    lines = f.readlines()
                    for line in lines[-5:]:
                        print(line.strip())
            except Exception:
                pass

    def display_next_steps(self):
        """ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´ (Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼)"""
        print(f"\n{self.YELLOW}ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:{self.NC}")
        print("1. ìˆ˜ì§‘ëœ ì™„ì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸ ë¶„ì„ ì§„í–‰")
        print("2. RDS ì¸ìŠ¤í„´ìŠ¤ ì„±ëŠ¥ ë° ë¹„ìš© ìµœì í™” ë¶„ì„")
        print("3. DynamoDB í…Œì´ë¸” êµ¬ì„± ë° ì„±ëŠ¥ ê²€í† ")
        print("4. ElastiCache í´ëŸ¬ìŠ¤í„° ìµœì í™” ë¶„ì„")
        print("5. ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ ë° ë¶„ì„ ì„œë¹„ìŠ¤ í™œìš©ë„ ë¶„ì„")
        print("6. ë¹…ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìµœì í™”")
        print("7. ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ë° ë³´ì•ˆ ì„¤ì • ì¢…í•© ê²€í† ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ì™„ì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ (RDS, NoSQL, ë¶„ì„ ì„œë¹„ìŠ¤ í¬í•¨)")
    parser.add_argument("--region", default=os.getenv("AWS_REGION", "ap-northeast-2"), help="AWS ë¦¬ì „")
    # ìŠ¤í¬ë¦½íŠ¸ì˜ ì‹¤ì œ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    default_report_dir = str(project_root / "aws-arch-analysis" / "report")
    
    parser.add_argument("--report-dir", default=os.getenv("REPORT_DIR", default_report_dir), help="ë³´ê³ ì„œ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    collector = SteampipeDatabaseCollector(args.region, args.report_dir)
    collector.collect_data()

if __name__ == "__main__":
    main()
