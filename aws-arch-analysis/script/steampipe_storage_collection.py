#!/usr/bin/env python3
"""
AWS ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ì¿¼ë¦¬ êµ¬ì¡° ì‚¬ìš©
"""

import subprocess
import json
import os
import sys
from pathlib import Path
from typing import List, Tuple
from datetime import datetime

class SteampipeStorageCollector:
    def __init__(self, region: str = "ap-northeast-2"):
        self.region = region
        self.report_dir = Path("/home/ec2-user/amazonqcli_lab/aws-arch-analysis/report")
        self.report_dir.mkdir(parents=True, exist_ok=True)
        self.total_count = 0
        self.success_count = 0
        self.error_log = self.report_dir / "storage_collection_errors.log"
        
        # ìƒ‰ìƒ ì½”ë“œ
        self.GREEN = '\033[0;32m'
        self.BLUE = '\033[0;34m'
        self.YELLOW = '\033[1;33m'
        self.RED = '\033[0;31m'
        self.NC = '\033[0m'  # No Color
        
    def log_info(self, message: str):
        print(f"{self.BLUE}â„¹ï¸ {message}{self.NC}")
        
    def log_success(self, message: str):
        print(f"{self.GREEN}âœ… {message}{self.NC}")
        
    def log_warning(self, message: str):
        print(f"{self.YELLOW}âš ï¸ {message}{self.NC}")
        
    def log_error(self, message: str):
        print(f"{self.RED}âŒ {message}{self.NC}")

    def check_steampipe_plugin(self):
        """Steampipe AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸"""
        self.log_info("Steampipe AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘...")
        try:
            result = subprocess.run(
                ["steampipe", "plugin", "list"],
                capture_output=True,
                text=True,
                check=True
            )
            if "aws" not in result.stdout:
                self.log_warning("AWS í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘...")
                subprocess.run(["steampipe", "plugin", "install", "aws"], check=True)
        except subprocess.CalledProcessError:
            self.log_warning("Steampipe í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")

    def execute_steampipe_query(self, description: str, query: str, output_file: str) -> bool:
        self.log_info(f"ìˆ˜ì§‘ ì¤‘: {description}")
        self.total_count += 1
        
        try:
            # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ report_dirë¡œ ë³€ê²½
            os.chdir(self.report_dir)
            
            result = subprocess.run(
                ["steampipe", "query", query, "--output", "json"],
                capture_output=True,
                text=True,
                check=True
            )
            
            output_path = self.report_dir / output_file
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(result.stdout)
            
            file_size = output_path.stat().st_size
            if file_size > 100:
                self.log_success(f"{description} ì™„ë£Œ ({output_file}, {file_size} bytes)")
                self.success_count += 1
                return True
            else:
                self.log_warning(f"{description} - ë°ì´í„° ì—†ìŒ ({output_file}, {file_size} bytes)")
                return False
                
        except subprocess.CalledProcessError as e:
            # ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ error_logì— ê¸°ë¡
            error_msg = f"{description} ì‹¤íŒ¨ - {output_file}"
            if e.stderr:
                error_msg += f": {e.stderr.strip()}"
            self.log_error(error_msg)
            
            # ì˜¤ë¥˜ ë¡œê·¸ì— ì¶”ê°€ ì •ë³´ ê¸°ë¡
            with open(self.error_log, 'a') as f:
                f.write(f"\nQuery failed: {query}\n")
                f.write(f"Error: {e.stderr}\n")
            
            return False

    def get_storage_queries(self) -> List[Tuple[str, str, str]]:
        """Shell ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼í•œ ì¿¼ë¦¬ êµ¬ì¡° ì‚¬ìš©"""
        return [
            # EBS ë³¼ë¥¨ ìƒì„¸ ì •ë³´
            (
                "EBS ë³¼ë¥¨ ìƒì„¸ ì •ë³´",
                f"select volume_id, volume_type, size, state, encrypted, kms_key_id, availability_zone, create_time, attachments, snapshot_id, iops, throughput, multi_attach_enabled, outpost_arn, fast_restored, tags from aws_ebs_volume where region = '{self.region}'",
                "storage_ebs_volumes.json"
            ),
            
            # EBS ìŠ¤ëƒ…ìƒ· ìƒì„¸ ì •ë³´
            (
                "EBS ìŠ¤ëƒ…ìƒ· ìƒì„¸ ì •ë³´",
                f"select snapshot_id, volume_id, volume_size, state, start_time, progress, owner_id, description, encrypted, kms_key_id, data_encryption_key_id, outpost_arn, storage_tier, restore_expiry_time, tags from aws_ebs_snapshot where region = '{self.region}' and owner_id = (select account_id from aws_caller_identity)",
                "storage_ebs_snapshots.json"
            ),
            
            # EBS ì•”í˜¸í™” ê¸°ë³¸ ì„¤ì •
            (
                "EBS ì•”í˜¸í™” ê¸°ë³¸ ì„¤ì •",
                f"select ebs_encryption_by_default, ebs_default_kms_key_id from aws_ec2_regional_settings where region = '{self.region}'",
                "storage_ebs_encryption_default.json"
            ),
            
            # S3 ë²„í‚· ìƒì„¸ ì •ë³´
            (
                "S3 ë²„í‚· ìƒì„¸ ì •ë³´",
                "select name, arn, region, creation_date, lifecycle_rules, logging, event_notification_configuration, object_lock_configuration, policy, policy_std, replication, server_side_encryption_configuration, versioning_enabled, versioning_mfa_delete, website_configuration, block_public_acls, block_public_policy, ignore_public_acls, restrict_public_buckets, tags from aws_s3_bucket",
                "storage_s3_buckets.json"
            ),
            
            # S3 ë²„í‚· ì •ì±…
            (
                "S3 ë²„í‚· ì •ì±…",
                "select name, policy, policy_std from aws_s3_bucket where policy is not null",
                "storage_s3_bucket_policies.json"
            ),
            
            # S3 ë²„í‚· í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤ ì°¨ë‹¨
            (
                "S3 ë²„í‚· í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤ ì°¨ë‹¨",
                "select name, block_public_acls, block_public_policy, ignore_public_acls, restrict_public_buckets from aws_s3_bucket",
                "storage_s3_public_access_block.json"
            ),
            
            # S3 ë²„í‚· CORS êµ¬ì„±
            (
                "S3 ë²„í‚· CORS êµ¬ì„±",
                "select name, cors_rules from aws_s3_bucket where cors_rules is not null",
                "storage_s3_cors.json"
            ),
            
            # S3 ë²„í‚· ìˆ˜ëª… ì£¼ê¸° êµ¬ì„±
            (
                "S3 ë²„í‚· ìˆ˜ëª… ì£¼ê¸° êµ¬ì„±",
                "select name, lifecycle_rules from aws_s3_bucket where lifecycle_rules is not null",
                "storage_s3_lifecycle.json"
            ),
            
            # S3 ë²„í‚· ë³µì œ êµ¬ì„±
            (
                "S3 ë²„í‚· ë³µì œ êµ¬ì„±",
                "select name, replication from aws_s3_bucket where replication is not null",
                "storage_s3_replication.json"
            ),
            
            # S3 ë²„í‚· ë²„ì „ ê´€ë¦¬
            (
                "S3 ë²„í‚· ë²„ì „ ê´€ë¦¬",
                "select name, versioning_enabled, versioning_mfa_delete from aws_s3_bucket",
                "storage_s3_versioning.json"
            ),
            
            # S3 ë²„í‚· ë¡œê¹…
            (
                "S3 ë²„í‚· ë¡œê¹…",
                "select name, logging from aws_s3_bucket where logging is not null",
                "storage_s3_logging.json"
            ),
            
            # S3 ë²„í‚· ì•Œë¦¼
            (
                "S3 ë²„í‚· ì•Œë¦¼",
                "select name, event_notification_configuration from aws_s3_bucket where event_notification_configuration is not null",
                "storage_s3_notifications.json"
            ),
            
            # S3 ë²„í‚· ì›¹ì‚¬ì´íŠ¸ êµ¬ì„±
            (
                "S3 ë²„í‚· ì›¹ì‚¬ì´íŠ¸ êµ¬ì„±",
                "select name, website_configuration from aws_s3_bucket where website_configuration is not null",
                "storage_s3_website.json"
            ),
            
            # S3 Glacier ë³¼íŠ¸
            (
                "S3 Glacier ë³¼íŠ¸",
                f"select vault_name, vault_arn, creation_date, last_inventory_date, number_of_archives, size_in_bytes, tags from aws_glacier_vault where region = '{self.region}'",
                "storage_glacier_vaults.json"
            ),
            
            # EFS íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„¸ ì •ë³´
            (
                "EFS íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„¸ ì •ë³´",
                f"select file_system_id, arn, creation_token, creation_time, life_cycle_state, name, number_of_mount_targets, owner_id, performance_mode, provisioned_throughput_in_mibps, throughput_mode, encrypted, kms_key_id, automatic_backups, replication_overwrite_protection, availability_zone_name, availability_zone_id, tags from aws_efs_file_system where region = '{self.region}'",
                "storage_efs_file_systems.json"
            ),
            
            # EFS ì•¡ì„¸ìŠ¤ í¬ì¸íŠ¸
            (
                "EFS ì•¡ì„¸ìŠ¤ í¬ì¸íŠ¸",
                f"select access_point_id, access_point_arn, file_system_id, posix_user, root_directory, client_token, life_cycle_state, name, owner_id, tags from aws_efs_access_point where region = '{self.region}'",
                "storage_efs_access_points.json"
            ),
            
            # EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ
            (
                "EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ",
                f"select mount_target_id, file_system_id, subnet_id, life_cycle_state, ip_address, network_interface_id, availability_zone_id, availability_zone_name, vpc_id, owner_id from aws_efs_mount_target where region = '{self.region}'",
                "storage_efs_mount_targets.json"
            ),
            
            # EFS ë°±ì—… ì •ì±…
            (
                "EFS ë°±ì—… ì •ì±…",
                f"select file_system_id, backup_policy from aws_efs_backup_policy where region = '{self.region}'",
                "storage_efs_backup_policies.json"
            ),
            
            # FSx íŒŒì¼ ì‹œìŠ¤í…œ
            (
                "FSx íŒŒì¼ ì‹œìŠ¤í…œ",
                f"select file_system_id, file_system_type, file_system_type_version, lifecycle, failure_details, storage_capacity, storage_type, vpc_id, subnet_ids, network_interface_ids, dns_name, kms_key_id, arn, tags, creation_time, lustre_configuration, ontap_configuration, open_zfs_configuration, windows_configuration from aws_fsx_file_system where region = '{self.region}'",
                "storage_fsx_file_systems.json"
            ),
            
            # FSx ë°±ì—…
            (
                "FSx ë°±ì—…",
                f"select backup_id, file_system_id, type, lifecycle, failure_details, progress_percent, creation_time, kms_key_id, resource_arn, tags, volume_id, source_backup_id, source_backup_region, resource_type, backup_type from aws_fsx_backup where region = '{self.region}'",
                "storage_fsx_backups.json"
            ),
            
            # Storage Gateway
            (
                "Storage Gateway",
                f"select gateway_id, gateway_name, gateway_timezone, gateway_type, gateway_state, ec2_instance_id, ec2_instance_region, host_environment, host_environment_id, endpoint_type, gateway_capacity, supported_gateway_capacities, deprecation_date, software_updates_end_date, tags from aws_storagegateway_gateway where region = '{self.region}'",
                "storage_gateway_gateways.json"
            ),
            
            # AWS Backup ë³¼íŠ¸
            (
                "AWS Backup ë³¼íŠ¸",
                f"select name, arn, creation_date, creator_request_id, number_of_recovery_points, locked, min_retention_days, max_retention_days, lock_date, encryption_key_arn from aws_backup_vault where region = '{self.region}'",
                "storage_backup_vaults.json"
            ),
            
            # AWS Backup ê³„íš
            (
                "AWS Backup ê³„íš",
                f"select backup_plan_id, arn, name, creation_date, deletion_date, last_execution_date, advanced_backup_settings, backup_plan from aws_backup_plan where region = '{self.region}'",
                "storage_backup_plans.json"
            ),
            
            # AWS Backup ì‘ì—…
            (
                "AWS Backup ì‘ì—…",
                f"select job_id, backup_vault_name, resource_arn, creation_date, completion_date, status, status_message, percent_done, backup_size, iam_role_arn, expected_completion_date, start_by, resource_type, bytes_transferred, backup_options, backup_type, parent_job_id, is_parent from aws_backup_job where region = '{self.region}'",
                "storage_backup_jobs.json"
            ),
            
            # Data Lifecycle Manager ì •ì±…
            (
                "Data Lifecycle Manager ì •ì±…",
                f"select policy_id, description, state, status_message, execution_role_arn, date_created, date_modified, policy_details, tags from aws_dlm_lifecycle_policy where region = '{self.region}'",
                "storage_dlm_policies.json"
            )
        ]

    def collect_data(self):
        """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        self.log_info("ğŸ’¾ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
        
        # Steampipe í”ŒëŸ¬ê·¸ì¸ í™•ì¸
        self.check_steampipe_plugin()
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        queries = self.get_storage_queries()
        for description, query, output_file in queries:
            self.execute_steampipe_query(description, query, output_file)
        
        # ê²°ê³¼ ìš”ì•½
        self.log_success("ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        self.log_info(f"ì„±ê³µ: {self.success_count}/{self.total_count}")
        
        # íŒŒì¼ ëª©ë¡ ë° í¬ê¸° í‘œì‹œ
        print(f"\n{self.BLUE}ğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:{self.NC}")
        for file_path in sorted(self.report_dir.glob("storage_*.json")):
            file_size = file_path.stat().st_size
            if file_size > 100:
                print(f"{self.GREEN}âœ“ {file_path.name} ({file_size} bytes){self.NC}")
            else:
                print(f"{self.YELLOW}âš  {file_path.name} ({file_size} bytes) - ë°ì´í„° ì—†ìŒ{self.NC}")
        
        # ìˆ˜ì§‘ í†µê³„
        print(f"\n{self.BLUE}ğŸ“Š ìˆ˜ì§‘ í†µê³„:{self.NC}")
        print(f"ì´ ì¿¼ë¦¬ ìˆ˜: {self.total_count}")
        print(f"ì„±ê³µí•œ ì¿¼ë¦¬: {self.success_count}")
        print(f"ì‹¤íŒ¨í•œ ì¿¼ë¦¬: {self.total_count - self.success_count}")
        print(f"ì„±ê³µë¥ : {(self.success_count/self.total_count*100):.1f}%")
        
        if self.error_log.exists():
            print(f"\n{self.YELLOW}âš ï¸ ì˜¤ë¥˜ ë¡œê·¸: {self.error_log}{self.NC}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    region = os.environ.get('AWS_DEFAULT_REGION', 'ap-northeast-2')
    collector = SteampipeStorageCollector(region)
    collector.collect_data()

if __name__ == "__main__":
    main()
