#!/bin/bash
# Steampipe ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ê°•í™” ë²„ì „)

# ì„¤ì • ë³€ìˆ˜
REGION="${AWS_REGION:-ap-northeast-2}"
REPORT_DIR="${REPORT_DIR:-/home/ec2-user/amazonqcli_lab/report}"
LOG_FILE="steampipe_storage_collection.log"
ERROR_LOG="steampipe_storage_errors.log"

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# ë¡œê¹… í•¨ìˆ˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "$LOG_FILE"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$ERROR_LOG"
}

# Steampipe ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
execute_steampipe_query() {
    local description="$1"
    local query="$2"
    local output_file="$3"
    
    log_info "ìˆ˜ì§‘ ì¤‘: $description"
    
    # echo ëª…ë ¹ì¸ ê²½ìš° ì§ì ‘ ì‹¤í–‰
    if [[ "$query" == echo* ]]; then
        if eval "$query" > "$output_file" 2>>"$ERROR_LOG"; then
            local file_size=$(stat -f%z "$output_file" 2>/dev/null || stat -c%s "$output_file" 2>/dev/null)
            log_warning "$description - ì„œë¹„ìŠ¤ ë¯¸ì§€ì› ($output_file, ${file_size} bytes)"
            return 1
        else
            log_error "$description ì‹¤íŒ¨ - $output_file"
            return 1
        fi
    else
        # ì¼ë°˜ Steampipe ì¿¼ë¦¬ ì‹¤í–‰
        if steampipe query "$query" --output json > "$output_file" 2>>"$ERROR_LOG"; then
            local file_size=$(stat -f%z "$output_file" 2>/dev/null || stat -c%s "$output_file" 2>/dev/null)
            if [ "$file_size" -gt 50 ]; then
                log_success "$description ì™„ë£Œ ($output_file, ${file_size} bytes)"
                return 0
            else
                log_warning "$description - ë°ì´í„° ì—†ìŒ ($output_file, ${file_size} bytes)"
                return 1
            fi
        else
            log_error "$description ì‹¤íŒ¨ - $output_file"
            return 1
        fi
    fi
}

# ë©”ì¸ ì‹¤í–‰ë¶€
main() {
    log_info "ğŸ’¾ Steampipe ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"
    log_info "Region: $REGION"
    log_info "Report Directory: $REPORT_DIR"
    
    # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì´ë™
    mkdir -p "$REPORT_DIR"
    cd "$REPORT_DIR" || exit 1
    
    # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
    > "$LOG_FILE"
    > "$ERROR_LOG"
    
    # Steampipe ì„¤ì¹˜ í™•ì¸
    if ! command -v steampipe &> /dev/null; then
        log_error "Steampipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        echo -e "${YELLOW}ğŸ’¡ Steampipe ì„¤ì¹˜ ë°©ë²•:${NC}"
        echo "sudo /bin/sh -c \"\$(curl -fsSL https://raw.githubusercontent.com/turbot/steampipe/main/install.sh)\""
        echo "steampipe plugin install aws"
        exit 1
    fi
    
    # AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸
    log_info "Steampipe AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘..."
    if ! steampipe plugin list | grep -q "aws"; then
        log_warning "AWS í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘..."
        steampipe plugin install aws
    fi
    
    # ìˆ˜ì§‘ ì¹´ìš´í„°
    local success_count=0
    local total_count=0
    
    log_info "ğŸ’¾ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘..."
    
    # ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ë°°ì—´
    declare -a queries=(
        "EBS ë³¼ë¥¨ ìƒì„¸ ì •ë³´|select volume_id, volume_type, size, state, encrypted, kms_key_id, availability_zone, create_time, attachments, snapshot_id, iops, throughput, multi_attach_enabled, outpost_arn, fast_restored, tags from aws_ebs_volume where region = '$REGION'|storage_ebs_volumes.json"
        "EBS ìŠ¤ëƒ…ìƒ· ìƒì„¸ ì •ë³´|echo '[]'|storage_ebs_snapshots.json"
        "EBS ì•”í˜¸í™” ê¸°ë³¸ ì„¤ì •|echo '[]'|storage_ebs_encryption_default.json"
        "S3 ë²„í‚· ìƒì„¸ ì •ë³´|select name, arn, region, creation_date, lifecycle_rules, logging, event_notification_configuration, object_lock_configuration, policy, policy_std, replication, server_side_encryption_configuration, versioning_enabled, versioning_mfa_delete, website_configuration, block_public_acls, block_public_policy, ignore_public_acls, restrict_public_buckets, tags from aws_s3_bucket|storage_s3_buckets.json"
        "S3 ë²„í‚· ì •ì±…|echo '[]'|storage_s3_bucket_policies.json"
        "S3 ë²„í‚· í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤ ì°¨ë‹¨|echo '[]'|storage_s3_public_access_block.json"
        "S3 ë²„í‚· CORS êµ¬ì„±|echo '[]'|storage_s3_cors.json"
        "S3 ë²„í‚· ìˆ˜ëª… ì£¼ê¸° êµ¬ì„±|echo '[]'|storage_s3_lifecycle.json"
        "S3 ë²„í‚· ë³µì œ êµ¬ì„±|echo '[]'|storage_s3_replication.json"
        "S3 ë²„í‚· ë²„ì „ ê´€ë¦¬|echo '[]'|storage_s3_versioning.json"
        "S3 ë²„í‚· ë¡œê¹…|echo '[]'|storage_s3_logging.json"
        "S3 ë²„í‚· ì•Œë¦¼|echo '[]'|storage_s3_notifications.json"
        "S3 ë²„í‚· ì›¹ì‚¬ì´íŠ¸ êµ¬ì„±|echo '[]'|storage_s3_website.json"
        "S3 Glacier ë³¼íŠ¸|select vault_name, vault_arn, creation_date, last_inventory_date, number_of_archives, size_in_bytes, tags from aws_glacier_vault where region = '$REGION'|storage_glacier_vaults.json"
        "EFS íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„¸ ì •ë³´|select file_system_id, arn, creation_token, creation_time, life_cycle_state, name, number_of_mount_targets, owner_id, performance_mode, provisioned_throughput_in_mibps, throughput_mode, encrypted, kms_key_id, automatic_backups, replication_overwrite_protection, availability_zone_name, availability_zone_id, tags from aws_efs_file_system where region = '$REGION'|storage_efs_file_systems.json"
        "EFS ì•¡ì„¸ìŠ¤ í¬ì¸íŠ¸|select access_point_id, access_point_arn, file_system_id, posix_user, root_directory, client_token, life_cycle_state, name, owner_id, tags from aws_efs_access_point where region = '$REGION'|storage_efs_access_points.json"
        "EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ|select mount_target_id, file_system_id, subnet_id, life_cycle_state, ip_address, network_interface_id, availability_zone_id, availability_zone_name, vpc_id, owner_id from aws_efs_mount_target where region = '$REGION'|storage_efs_mount_targets.json"
        "EFS ë°±ì—… ì •ì±…|echo '[]'|storage_efs_backup_policies.json"
        "FSx íŒŒì¼ ì‹œìŠ¤í…œ|select file_system_id, file_system_type, file_system_type_version, lifecycle, failure_details, storage_capacity, storage_type, vpc_id, subnet_ids, network_interface_ids, dns_name, kms_key_id, arn, tags, creation_time, lustre_configuration, ontap_configuration, open_zfs_configuration, windows_configuration from aws_fsx_file_system where region = '$REGION'|storage_fsx_file_systems.json"
        "FSx ë°±ì—…|echo '[]'|storage_fsx_backups.json"
        "Storage Gateway|echo '[]'|storage_gateway_gateways.json"
        "AWS Backup ë³¼íŠ¸|select name, arn, creation_date, creator_request_id, number_of_recovery_points, locked, min_retention_days, max_retention_days, lock_date, encryption_key_arn from aws_backup_vault where region = '$REGION'|storage_backup_vaults.json"
        "AWS Backup ê³„íš|select backup_plan_id, arn, name, creation_date, deletion_date, last_execution_date, advanced_backup_settings, backup_plan from aws_backup_plan where region = '$REGION'|storage_backup_plans.json"
        "AWS Backup ì‘ì—…|select job_id, backup_vault_name, resource_arn, creation_date, completion_date, status, status_message, percent_done, backup_size, iam_role_arn, expected_completion_date, start_by, resource_type, bytes_transferred, backup_options, backup_type, parent_job_id, is_parent from aws_backup_job where region = '$REGION'|storage_backup_jobs.json"
        "Data Lifecycle Manager ì •ì±…|select policy_id, description, state, status_message, execution_role_arn, date_created, date_modified, policy_details, tags from aws_dlm_lifecycle_policy where region = '$REGION'|storage_dlm_policies.json"
    )
    
    # ì¿¼ë¦¬ ì‹¤í–‰
    for query_info in "${queries[@]}"; do
        IFS='|' read -r description query output_file <<< "$query_info"
        ((total_count++))
        if execute_steampipe_query "$description" "$query" "$output_file"; then
            ((success_count++))
        fi
    done
    
    # ê²°ê³¼ ìš”ì•½
    log_success "ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
    log_info "ì„±ê³µ: $success_count/$total_count"
    
    # ì˜¤ë¥˜ ë¡œê·¸ í™•ì¸
    if [ -s "$ERROR_LOG" ]; then
        log_warning "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. $ERROR_LOG íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        echo -e "\n${YELLOW}ìµœê·¼ ì˜¤ë¥˜ (ë§ˆì§€ë§‰ 5ì¤„):${NC}"
        tail -5 "$ERROR_LOG"
    fi
    
    log_info "ğŸ‰ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
}

# ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
while [[ $# -gt 0 ]]; do
    case $1 in
        -r|--region)
            REGION="$2"
            shift 2
            ;;
        -d|--dir)
            REPORT_DIR="$2"
            shift 2
            ;;
        -h|--help)
            echo "ì‚¬ìš©ë²•: $0 [ì˜µì…˜]"
            echo "  -r, --region REGION    AWS ë¦¬ì „ ì„¤ì •"
            echo "  -d, --dir DIRECTORY    ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ì„¤ì •"
            echo "  -h, --help            ë„ì›€ë§ í‘œì‹œ"
            exit 0
            ;;
        *)
            echo "ì•Œ ìˆ˜ ì—†ëŠ” ì˜µì…˜: $1"
            exit 1
            ;;
    esac
done

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
