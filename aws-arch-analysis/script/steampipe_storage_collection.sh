#!/bin/bash
# Steampipe ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ê°•í™” ë²„ì „)

# ì„¤ì • ë³€ìˆ˜
REGION="${AWS_REGION:-ap-northeast-2}"
REPORT_DIR="${REPORT_DIR:-/home/ec2-user/amazonqcli_lab/report}"
LOG_FILE="steampipe_storage_collection.log"
ERROR_LOG="steampipe_storage_errors.log"

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

# ë¡œê¹… í•¨ìˆ˜
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1" | tee -a "$LOG_FILE"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1" | tee -a "$LOG_FILE"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1" | tee -a "$LOG_FILE"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1" | tee -a "$ERROR_LOG"
}

# Steampipe ì¿¼ë¦¬ ì‹¤í–‰ í•¨ìˆ˜
execute_steampipe_query() {
    local description="$1"
    local query="$2"
    local output_file="$3"
    
    log_info "ìˆ˜ì§‘ ì¤‘: $description"
    
    if steampipe query "$query" --output json > "$output_file" 2>>"$ERROR_LOG"; then
        local file_size=$(stat -f%z "$output_file" 2>/dev/null || stat -c%s "$output_file" 2>/dev/null)
        if [ "$file_size" -gt 50 ]; then
            log_success "$description ì™„ë£Œ ($output_file, ${file_size} bytes)"
            return 0
        else
            log_warning "$description - ë°ì´í„° ì—†ìŒ ($output_file, ${file_size} bytes)"
            return 1
        fi
    else
        log_error "$description ì‹¤íŒ¨ - $output_file"
        return 1
    fi
}

# ë©”ì¸ ì‹¤í–‰ë¶€
main() {
    log_info "ğŸ’¾ Steampipe ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"
    log_info "Region: $REGION"
    log_info "Report Directory: $REPORT_DIR"
    
    # ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì´ë™
    mkdir -p "$REPORT_DIR"
    cd "$REPORT_DIR" || exit 1
    
    # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
    > "$LOG_FILE"
    > "$ERROR_LOG"
    
    # Steampipe ì„¤ì¹˜ í™•ì¸
    if ! command -v steampipe &> /dev/null; then
        log_error "Steampipeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        echo -e "${YELLOW}ğŸ’¡ Steampipe ì„¤ì¹˜ ë°©ë²•:${NC}"
        echo "sudo /bin/sh -c \"\$(curl -fsSL https://raw.githubusercontent.com/turbot/steampipe/main/install.sh)\""
        echo "steampipe plugin install aws"
        exit 1
    fi
    
    # AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸
    log_info "Steampipe AWS í”ŒëŸ¬ê·¸ì¸ í™•ì¸ ì¤‘..."
    if ! steampipe plugin list | grep -q "aws"; then
        log_warning "AWS í”ŒëŸ¬ê·¸ì¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜ ì¤‘..."
        steampipe plugin install aws
    fi
    
    # ìˆ˜ì§‘ ì¹´ìš´í„°
    local success_count=0
    local total_count=0
    
    log_info "ğŸ’¾ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ì‹œì‘..."
    
    # ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ìˆ˜ì§‘ ë°°ì—´
    declare -a storage_queries=(
        "EBS ë³¼ë¥¨ ìƒì„¸ ì •ë³´|select volume_id, volume_type, size, state, encrypted, kms_key_id, availability_zone, create_time, attachments, snapshot_id, iops, throughput, multi_attach_enabled, outpost_arn, fast_restored, tags from aws_ebs_volume where region = '$REGION'|storage_ebs_volumes.json"
        "EBS ìŠ¤ëƒ…ìƒ· ìƒì„¸ ì •ë³´|select snapshot_id, volume_id, volume_size, state, start_time, progress, owner_id, description, encrypted, kms_key_id, data_encryption_key_id, outpost_arn, storage_tier, restore_expiry_time, tags from aws_ebs_snapshot where region = '$REGION' and owner_id = (select account_id from aws_caller_identity)|storage_ebs_snapshots.json"
        "EBS ì•”í˜¸í™” ê¸°ë³¸ ì„¤ì •|select ebs_encryption_by_default, ebs_default_kms_key_id from aws_ec2_regional_settings where region = '$REGION'|storage_ebs_encryption_default.json"
        "S3 ë²„í‚· ìƒì„¸ ì •ë³´|select name, arn, region, creation_date, lifecycle_rules, logging, event_notification_configuration, object_lock_configuration, policy, policy_std, replication, server_side_encryption_configuration, versioning_enabled, versioning_mfa_delete, website_configuration, block_public_acls, block_public_policy, ignore_public_acls, restrict_public_buckets, tags from aws_s3_bucket|storage_s3_buckets.json"
        "S3 ë²„í‚· ì •ì±…|select name, policy, policy_std from aws_s3_bucket where policy is not null|storage_s3_bucket_policies.json"
        "S3 ë²„í‚· í¼ë¸”ë¦­ ì•¡ì„¸ìŠ¤ ì°¨ë‹¨|select name, block_public_acls, block_public_policy, ignore_public_acls, restrict_public_buckets from aws_s3_bucket|storage_s3_public_access_block.json"
        "S3 ë²„í‚· CORS êµ¬ì„±|select name, cors_rules from aws_s3_bucket where cors_rules is not null|storage_s3_cors.json"
        "S3 ë²„í‚· ìˆ˜ëª… ì£¼ê¸° êµ¬ì„±|select name, lifecycle_rules from aws_s3_bucket where lifecycle_rules is not null|storage_s3_lifecycle.json"
        "S3 ë²„í‚· ë³µì œ êµ¬ì„±|select name, replication from aws_s3_bucket where replication is not null|storage_s3_replication.json"
        "S3 ë²„í‚· ë²„ì „ ê´€ë¦¬|select name, versioning_enabled, versioning_mfa_delete from aws_s3_bucket|storage_s3_versioning.json"
        "S3 ë²„í‚· ë¡œê¹…|select name, logging from aws_s3_bucket where logging is not null|storage_s3_logging.json"
        "S3 ë²„í‚· ì•Œë¦¼|select name, event_notification_configuration from aws_s3_bucket where event_notification_configuration is not null|storage_s3_notifications.json"
        "S3 ë²„í‚· ì›¹ì‚¬ì´íŠ¸ êµ¬ì„±|select name, website_configuration from aws_s3_bucket where website_configuration is not null|storage_s3_website.json"
        "S3 Glacier ë³¼íŠ¸|select vault_name, vault_arn, creation_date, last_inventory_date, number_of_archives, size_in_bytes, tags from aws_glacier_vault where region = '$REGION'|storage_glacier_vaults.json"
        "EFS íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„¸ ì •ë³´|select file_system_id, arn, creation_token, creation_time, life_cycle_state, name, number_of_mount_targets, owner_id, performance_mode, provisioned_throughput_in_mibps, throughput_mode, encrypted, kms_key_id, automatic_backups, replication_overwrite_protection, availability_zone_name, availability_zone_id, tags from aws_efs_file_system where region = '$REGION'|storage_efs_file_systems.json"
        "EFS ì•¡ì„¸ìŠ¤ í¬ì¸íŠ¸|select access_point_id, access_point_arn, file_system_id, posix_user, root_directory, client_token, life_cycle_state, name, owner_id, tags from aws_efs_access_point where region = '$REGION'|storage_efs_access_points.json"
        "EFS ë§ˆìš´íŠ¸ íƒ€ê²Ÿ|select mount_target_id, file_system_id, subnet_id, life_cycle_state, ip_address, network_interface_id, availability_zone_id, availability_zone_name, vpc_id, owner_id from aws_efs_mount_target where region = '$REGION'|storage_efs_mount_targets.json"
        "EFS ë°±ì—… ì •ì±…|select file_system_id, backup_policy from aws_efs_backup_policy where region = '$REGION'|storage_efs_backup_policies.json"
        "FSx íŒŒì¼ ì‹œìŠ¤í…œ|select file_system_id, file_system_type, file_system_type_version, lifecycle, failure_details, storage_capacity, storage_type, vpc_id, subnet_ids, network_interface_ids, dns_name, kms_key_id, arn, tags, creation_time, lustre_configuration, ontap_configuration, open_zfs_configuration, windows_configuration from aws_fsx_file_system where region = '$REGION'|storage_fsx_file_systems.json"
        "FSx ë°±ì—…|select backup_id, file_system_id, type, lifecycle, failure_details, progress_percent, creation_time, kms_key_id, resource_arn, tags, volume_id, source_backup_id, source_backup_region, resource_type, backup_type from aws_fsx_backup where region = '$REGION'|storage_fsx_backups.json"
        "Storage Gateway|select gateway_id, gateway_name, gateway_timezone, gateway_type, gateway_state, ec2_instance_id, ec2_instance_region, host_environment, host_environment_id, endpoint_type, gateway_capacity, supported_gateway_capacities, deprecation_date, software_updates_end_date, tags from aws_storagegateway_gateway where region = '$REGION'|storage_gateway_gateways.json"
        "AWS Backup ë³¼íŠ¸|select name, arn, creation_date, creator_request_id, number_of_recovery_points, locked, min_retention_days, max_retention_days, lock_date, encryption_key_arn from aws_backup_vault where region = '$REGION'|storage_backup_vaults.json"
        "AWS Backup ê³„íš|select backup_plan_id, arn, name, creation_date, deletion_date, last_execution_date, advanced_backup_settings, backup_plan from aws_backup_plan where region = '$REGION'|storage_backup_plans.json"
        "AWS Backup ì‘ì—…|select job_id, backup_vault_name, resource_arn, creation_date, completion_date, status, status_message, percent_done, backup_size, iam_role_arn, expected_completion_date, start_by, resource_type, bytes_transferred, backup_options, backup_type, parent_job_id, is_parent from aws_backup_job where region = '$REGION'|storage_backup_jobs.json"
        "Data Lifecycle Manager ì •ì±…|select policy_id, description, state, status_message, execution_role_arn, date_created, date_modified, policy_details, tags from aws_dlm_lifecycle_policy where region = '$REGION'|storage_dlm_policies.json"
    )
    
    # ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ì¿¼ë¦¬ ì‹¤í–‰
    for query_info in "${storage_queries[@]}"; do
        IFS='|' read -r description query output_file <<< "$query_info"
        ((total_count++))
        if execute_steampipe_query "$description" "$query" "$output_file"; then
            ((success_count++))
        fi
    done
    
    # ê²°ê³¼ ìš”ì•½
    log_success "ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!"
    log_info "ì„±ê³µ: $success_count/$total_count"
    
    # íŒŒì¼ ëª©ë¡ ë° í¬ê¸° í‘œì‹œ
    echo -e "\n${BLUE}ğŸ“ ìƒì„±ëœ íŒŒì¼ ëª©ë¡:${NC}"
    for file in storage_*.json; do
        if [ -f "$file" ]; then
            size=$(stat -f%z "$file" 2>/dev/null || stat -c%s "$file" 2>/dev/null)
            if [ "$size" -gt 100 ]; then
                echo -e "${GREEN}âœ“ $file (${size} bytes)${NC}"
            else
                echo -e "${YELLOW}âš  $file (${size} bytes) - ë°ì´í„° ì—†ìŒ${NC}"
            fi
        fi
    done
    
    # ìˆ˜ì§‘ í†µê³„
    echo -e "\n${BLUE}ğŸ“Š ìˆ˜ì§‘ í†µê³„:${NC}"
    echo "ì´ ì¿¼ë¦¬ ìˆ˜: $total_count"
    echo "ì„±ê³µí•œ ì¿¼ë¦¬: $success_count"
    echo "ì‹¤íŒ¨í•œ ì¿¼ë¦¬: $((total_count - success_count))"
    
    # ì˜¤ë¥˜ ë¡œê·¸ í™•ì¸
    if [ -s "$ERROR_LOG" ]; then
        log_warning "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. $ERROR_LOG íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."
        echo -e "\n${YELLOW}ìµœê·¼ ì˜¤ë¥˜ (ë§ˆì§€ë§‰ 5ì¤„):${NC}"
        tail -5 "$ERROR_LOG"
    fi
    
    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    echo -e "\n${YELLOW}ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:${NC}"
    echo "1. ìˆ˜ì§‘ëœ ìŠ¤í† ë¦¬ì§€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ Phase 1 ì¸í”„ë¼ ë¶„ì„ ì§„í–‰"
    echo "2. EBS ë³¼ë¥¨ ì„±ëŠ¥ ë° ë¹„ìš© ìµœì í™” ë¶„ì„"
    echo "3. S3 ë²„í‚· ë³´ì•ˆ ì„¤ì • ë° ìˆ˜ëª… ì£¼ê¸° ì •ì±… ê²€í† "
    echo "4. EFS ë° FSx íŒŒì¼ ì‹œìŠ¤í…œ ì„±ëŠ¥ ë¶„ì„"
    echo "5. ë°±ì—… ì „ëµ ë° ë°ì´í„° ë³´í˜¸ ì •ì±… ê²€í† "
    
    log_info "ğŸ‰ ìŠ¤í† ë¦¬ì§€ ë¦¬ì†ŒìŠ¤ ë°ì´í„° ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
}

# ë„ì›€ë§ í•¨ìˆ˜
show_help() {
    echo "ì‚¬ìš©ë²•: $0 [ì˜µì…˜]"
    echo ""
    echo "ì˜µì…˜:"
    echo "  -r, --region REGION    AWS ë¦¬ì „ ì„¤ì • (ê¸°ë³¸ê°’: ap-northeast-2)"
    echo "  -d, --dir DIRECTORY    ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ì„¤ì • (ê¸°ë³¸ê°’: ~/report)"
    echo "  -h, --help            ì´ ë„ì›€ë§ í‘œì‹œ"
    echo ""
    echo "í™˜ê²½ ë³€ìˆ˜:"
    echo "  AWS_REGION            AWS ë¦¬ì „ ì„¤ì •"
    echo "  REPORT_DIR            ë³´ê³ ì„œ ë””ë ‰í† ë¦¬ ì„¤ì •"
    echo ""
    echo "ì˜ˆì‹œ:"
    echo "  $0                                    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰"
    echo "  $0 -r us-east-1                      # íŠ¹ì • ë¦¬ì „ìœ¼ë¡œ ì‹¤í–‰"
    echo "  $0 -d /custom/path                   # ì‚¬ìš©ì ì •ì˜ ë””ë ‰í† ë¦¬ë¡œ ì‹¤í–‰"
    echo "  AWS_REGION=eu-west-1 $0              # í™˜ê²½ ë³€ìˆ˜ë¡œ ë¦¬ì „ ì„¤ì •"
}

# ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
while [[ $# -gt 0 ]]; do
    case $1 in
        -r|--region)
            REGION="$2"
            shift 2
            ;;
        -d|--dir)
            REPORT_DIR="$2"
            shift 2
            ;;
        -h|--help)
            show_help
            exit 0
            ;;
        *)
            echo "ì•Œ ìˆ˜ ì—†ëŠ” ì˜µì…˜: $1"
            show_help
            exit 1
            ;;
    esac
done

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
